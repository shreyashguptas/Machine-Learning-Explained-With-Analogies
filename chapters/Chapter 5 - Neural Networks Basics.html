
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 5 - Neural Networks Basics &#8212; Machine Learning for Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=d2032c04" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/theme.css?v=a243ae73" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/Chapter 5 - Neural Networks Basics';</script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/searchtools.js?v=63a53a7d"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/language_data.js?v=d4673a71"></script>
    <script src="../_static/copybutton_funcs.js?v=776a791e"></script>
    <script src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/scripts/bootstrap.js?v=7583a70d"></script>
    <script src="../_static/scripts/fontawesome.js?v=9b125980"></script>
    <script src="../_static/scripts/pydata-sphinx-theme.js?v=f62441ba"></script>
    <link rel="icon" href="../_static/course-logo.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6 - Deep Learning Tools" href="Chapter%206%20-%20Deep%20Learning%20Tools.html" />
    <link rel="prev" title="Chapter 4 - Unsupervised Learning" href="Chapter%204%20-%20Unsupervised%20Learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/course-logo.svg" class="logo__image only-light" alt="Machine Learning for Dummies - Home"/>
    <img src="../_static/course-logo.svg" class="logo__image only-dark pst-js-only" alt="Machine Learning for Dummies - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Machine Learning for Dummies 🧠
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations of Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%201%20-%20Introduction%20to%20Machine%20Learning.html">Chapter 1: Introduction to Machine Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%202%20-%20Data%20Fundamentals.html">Data Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter%203%20-%20Supervised%20Learning.html">Supervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter%204%20-%20Unsupervised%20Learning.html">Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 5 - Neural Networks Basics</a></li>




















<li class="toctree-l1"><a class="reference internal" href="Chapter%206%20-%20Deep%20Learning%20Tools.html">Deep Learning Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%207%20-%20Convolutional%20Neural%20Networks.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter%208%20-%20Sequential%20Data%20and%20RNNs.html">Sequential Data and RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter%209%20-%20Modern%20Deep%20Learning.html">Modern Deep Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/shreyashguptas/Machine-Learning-for-Dummies/blob/main/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/edit/main/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/issues/new?title=Issue%20on%20page%20%2Fchapters/Chapter 5 - Neural Networks Basics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 5 - Neural Networks Basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 5 - Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">Neural Network Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-torch-library">Importing the Torch Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-attributes">Tensor Attributes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensor-attributes">What are tensor attributes?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shape">1. Tensor Shape</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-data-type">2. Tensor Data Type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-device">3. Tensor Device</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-our-own-neural-network">Creating our own Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-to-know-the-linear-layer-operation">Getting to know the linear layer operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-visualize-the-linear-layers-with-their-weights-and-biases">Lets visualize the Linear_layers with their weights and biases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ignore-the-code-block-below-it-is-just-for-visualization">Ignore the code block below, it is just for visualization</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#each-nn-linear-line-of-code-is-a-linear-layer">Each <strong>nn.Linear</strong> line of code is a Linear Layer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-layers-with-nn-sequential">Stacking layers with nn.Sequential()</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise #1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-first-neural-network">Your first neural network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-linear-layers">Stacking linear layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#there-are-multiple-types-of-layers">There are multiple types of Layers</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-activation-functions">Why do we need activation functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function-example-with-an-activation-layer">Sigmoid function example with an Activation layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function-limitations">Sigmoid Function Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-each">When to Use Each</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-as-an-activation-layer">Softmax as an Activation Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function-limitations">Softmax Function Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#better-alternatives">Better Alternatives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">When to Use Each</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-6">Exercise 1.6</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-and-softmax-functions">The sigmoid and softmax functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-1-2">Instructions 1/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-2-2">Instruction 2/2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-and-backpropagation">Forward Propagation and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-forward-propagation">What is Forward Propagation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-used-and-important">Why is it Used and Important?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-analogy-for-forward-propagation">An Analogy for Forward Propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-backpropagation">What is Backpropagation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Why is it Used and Important?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-analogy-for-backpropagation">An Analogy for Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-differences-between-terms">Understanding the Differences Between Terms</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-vs-regression-problems-in-deep-learning">Classification vs Regression Problems in Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-continuous-output">Regression (Continuous Output)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-discrete-output">Classification (Discrete Output)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-in-forward-pass">Key Differences in Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-real-world-example">Simple Real-World Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification-forward-propagation">Binary Classification: Forward Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-video-game-character-selection-analogy">The Video Game Character Selection Analogy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-station-input-layer">Input Station (Input Layer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magic-processing-stations-hidden-layers">Magic Processing Stations (Hidden Layers)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-decision-station-output-layer">Final Decision Station (Output Layer)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-classification-forward-pass">Multi-class classification: forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-school-club-assignment-system">The School Club Assignment System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-assignment-process-forward-pass">The Assignment Process (Forward Pass)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-of-multi-class-classification-from-binary-classification">Key Differences of Multi-class classification from Binary Classification:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-s-important">Why It’s Important:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-forward-propogation">Regression: Forward Propogation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-1">Exercise 2.1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-binary-classifier-in-pytorch">Building a binary classifier in PyTorch</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-2">Exercise 2.2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-regression-to-multi-class-classification">From regression to multi-class classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Instructions 1/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-2-2">Instructions 2/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-for-the-code-above">Explanation for the code above</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#using-loss-functions-to-assess-model-predictions">Using Loss Functions to assess model predictions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-loss-function">What is a Loss Function?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-loss-functions">Why Do We Need Loss Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-loss-functions">Types of Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-loss-functions">Regression Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-in-a-formula">Loss Function in a formula</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One Hot encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#here-s-how-it-works">Here’s how it works:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-use-one-hot-encoding">Why do we use One Hot encoding?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-takes">Loss function takes:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-outputs">Loss function outputs:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#our-training-goal-is-to-minimize-this-loss-the-lower-the-loss-the-better-the-model">✨ <strong>Our training goal is to minimize this loss!</strong> The lower the loss, the better the model! ✨</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-3">Exercise 2.3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-one-hot-encoded-labels">Creating one-hot encoded labels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-4">Exercise 2.4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-cross-entropy-loss">Calculating cross entropy loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-1-3">Instruction 1/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-2-3">Instruction 2/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-3-3">Instruction 3/3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#using-derivatives-to-update-model-parameters-a-treasure-hunt-analogy">Using Derivatives to Update Model Parameters: A Treasure Hunt Analogy</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-treasure-hunt-game-an-analogy-for-gradient-descent">The Treasure Hunt Game: An Analogy for Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#game-rules">Game Rules:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mapping-the-game-to-machine-learning-concepts">Mapping the Game to Machine Learning Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-park-the-loss-landscape">The Park = The “Loss Landscape”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#you-the-learning-algorithm">You = The Learning Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feeling-the-ground-calculating-gradients">Feeling the Ground = Calculating Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#taking-steps-updating-the-model">Taking Steps = Updating the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-treasure-the-best-model">The Treasure = The Best Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-concepts">Important Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-step-size">Learning Rate = Step Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-minima-small-dips">Local Minima = Small Dips</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-finding-the-treasure">Convergence = Finding the Treasure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-is-fascinating">Why this is Fascinating</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#below-is-an-example-of-backpropagation">Below is an example of Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">Code Breakdown</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">1. Data Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">2. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">3. Forward Pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-calculation-and-backpropagation">4. Loss Calculation and Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-gradients">5. Accessing Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-output-of-the-code-above">Understanding the Output of the code above ⬆︎</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-part-weight-gradients">First Part (Weight Gradients)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-part-bias-gradients">Second Part (Bias Gradients)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-analogy">Real-world Analogy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#most-popular-pytorch-optimizers">Most Popular PyTorch Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-optimizers">How to Use Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-optimizer">Choosing the Right Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-for-using-optimizers">Tips for Using Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-a-sample">Estimating a sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Exercise</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-the-model-parameters">Accessing the model parameters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-weights-manually">Updating the weights manually</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-pytorch-optimizer">Using the PyTorch optimizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-our-first-training-loop">Writing our first Training Loop!</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-neural-network">Training a neural network</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introducting-the-mean-squared-error-loss">Introducting the Mean Squared Error Loss</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-mse-with-a-pizza-delivery-analogy">Understanding MSE with a Pizza Delivery Analogy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-implementation">PyTorch Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-mseloss">Using the MSELoss</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#some-terminology-before-we-move-forward">Some terminology before we move forward</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epoch">Epoch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-features">batch_features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-labels">batch_labels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-dataloader">The dataloader</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-a-training-loop">Writing a training loop</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-1-3">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-2-3">Instructions (2/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-3-3">Instructions (3/3)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-neural-network-architecture-and-hyperparameters">Section 3 - Neural Network Architecture and Hyperparameters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-relu-rectified-linear-unit-and-leakyrelu">Introducing ReLU (Rectified Linear Unit) and LeakyReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">Leaky ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-relu">Implementing ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-leaky-relu">Implementing leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-counting">Parameter Counting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-count-the-parameters-in-a-neural-network-using-a-simple-example">Let’s count the parameters in a neural network using a simple example:</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#here-s-the-code-we-ll-analyze">Here’s the code we’ll analyze:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-restaurant-kitchen-model">The Restaurant Kitchen Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Instructions (2/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-the-code-above-does">What the code above does</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-parameter-counter">Understanding the Parameter Counter</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-the-capacity-of-a-network">Manipulating the capacity of a network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-momentum-with-sgd-optimizer-stocahastic-gradient-descent">Learning Rate and Momentum with SGD Optimizer (Stocahastic Gradient Descent)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-skiing-adventure-of-sgd">The Skiing Adventure of SGD</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-your-skiing-speed">Learning Rate: Your Skiing Speed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-momentum-the-skiing-inertia">Understanding Momentum: The Skiing Inertia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-momentum-0-9-0-99">High Momentum (0.9 - 0.99)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#low-momentum-0-1-0-5">Low Momentum (0.1 - 0.5)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-momentum-0-0">Zero Momentum (0.0)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-scenarios">Real-World Scenarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-momentum-values-and-their-effects">Common Momentum Values and Their Effects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips">Practical Tips</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-right-balance">Finding the Right Balance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimenting-with-momentum">Experimenting with momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-initialization-and-transfer-learning">Layer Initialization and Transfer Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-initialization">Layer Initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-process">Fine-tuning process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example-of-freezing-layers-of-a-model">Code example of Freezing layers of a model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Layer initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Instructions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-data">Loading Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Instructions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-data-loading-to-running-a-forward-pass">From data loading to running a forward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Instructions 2/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Instructions 3/3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-stages-of-learning-training-validation-and-testing">The Three Stages of Learning: Training, Validation and Testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-split-is-important">Why This Split is Important</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-example">Real-World Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-the-evaluation-loop">Writing the evaluation loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-accuracy-using-torchmetrics">Calculating accuracy using torchmetrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Instructions (2/2)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting-models">Overfitting and Underfitting Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-story-of-three-students">The Story of Three Students</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-meet-mary-the-memorizer">Overfitting: Meet Mary the Memorizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting-meet-johnny-the-generalizer">Underfitting: Meet Johnny the Generalizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-right-meet-sarah-the-smart-learner">Just Right: Meet Sarah the Smart Learner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-matters">Why It Matters</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#fighting-overfitting">Fighting Overfitting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-decay-the-lazy-student-method">Weight Decay: The “Lazy Student” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-augmentation-the-study-buddy-method">Data Augmentation: The “Study Buddy” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-the-pop-quiz-method">Dropout: The “Pop Quiz” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-all-three">Why Use All Three?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimenting-with-dropout">Experimenting with dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Instructions (2/2)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-model-performance">Improving Model Performance</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-establish-a-baseline">Step 1: Establish a Baseline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-reduce-overfitting">Step 2: Reduce Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-optimize-hyperparameters">Step 3: Optimize Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-random-search">Implementing random search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Instructions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/shreyashguptas/Machine-Learning-for-Dummies/blob/main/chapters/Chapter%20x%20-%20Introduction%20to%20Deep%20Learning.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-5-neural-networks-basics">
<h1>Chapter 5 - Neural Networks Basics<a class="headerlink" href="#chapter-5-neural-networks-basics" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p>Course link: <a class="reference external" href="https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch">https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch</a></p>
<blockquote>
<div><h3 class="rubric" id="note-the-basic-pre-requisites-for-this-deep-learning-part-is-that-you-have-gone-through-the-machine-learning-notebook-before-doing-deep-learning">Note: The basic pre-requisites for this Deep Learning part is that you have gone through the Machine Learning notebook before doing Deep Learning.</h3>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># %pip install pandas numpy torch matplotlib</span>
</pre></div>
</div>
</div>
</div>
<section id="neural-network-overview">
<h2>Neural Network Overview<a class="headerlink" href="#neural-network-overview" title="Link to this heading">#</a></h2>
<p>Here is how a very basic neural network looks like. It’s inspired by the human brain and that’s why we call it neurons and neural networks.</p>
<p><img alt="neural-network-overview" src="../_images/neural-network-overview.png" /></p>
</section>
<section id="importing-the-torch-library">
<h2>Importing the Torch Library<a class="headerlink" href="#importing-the-torch-library" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>to import the torch library all you have to do is run the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tensor-attributes">
<h2>Tensor Attributes<a class="headerlink" href="#tensor-attributes" title="Link to this heading">#</a></h2>
<section id="what-are-tensor-attributes">
<h3>What are tensor attributes?<a class="headerlink" href="#what-are-tensor-attributes" title="Link to this heading">#</a></h3>
<p>Tensors are the basic building blocks of data in PyTorch. They are similar to arrays or matrices, but with some additional features that make them useful for deep learning tasks.</p>
<p>Tensor as different attributes</p>
<ol class="arabic simple">
<li><p>Tensor Shape</p></li>
<li><p>Tensor Data Type</p></li>
<li><p>Tensor Device</p></li>
</ol>
</section>
<section id="tensor-shape">
<h3>1. Tensor Shape<a class="headerlink" href="#tensor-shape" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Lst</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Lst</span><span class="p">)</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 3])
</pre></div>
</div>
</div>
</div>
</section>
<section id="tensor-data-type">
<h3>2. Tensor Data Type<a class="headerlink" href="#tensor-data-type" title="Link to this heading">#</a></h3>
<p>PyTorch supports various data types for tensors, categorized as follows:</p>
<p>Floating-point Types:
• torch.float32 or torch.float: 32-bit floating-point
• torch.float64 or torch.double: 64-bit floating-point
• torch.float16 or torch.half: 16-bit floating-point
• torch.bfloat16: Brain floating-point (optimized for machine learning)</p>
<p>Integer Types:
• torch.int8: 8-bit signed integer
• torch.uint8: 8-bit unsigned integer
• torch.int16 or torch.short: 16-bit signed integer
• torch.int32 or torch.int: 32-bit signed integer
• torch.int64 or torch.long: 64-bit signed integer</p>
<p>Boolean Type:
• torch.bool: Boolean (True or False)</p>
<p>Complex Types:
• torch.complex64: Complex number (32-bit float for real and imaginary parts)
• torch.complex128 or torch.cdouble: Complex number (64-bit float for real and imaginary parts)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="tensor-device">
<h3>3. Tensor Device<a class="headerlink" href="#tensor-device" title="Link to this heading">#</a></h3>
<p>When you run tensor.device on a PyTorch tensor, it returns the device where the tensor is stored. Common device types include:</p>
<ol class="arabic simple">
<li><p>CPU: Tensor is in system memory, processed by the CPU.
Example: device(type=’cpu’)</p></li>
<li><p>CUDA: Tensor is in GPU memory, processed by an NVIDIA GPU.
Examples:</p>
<ul class="simple">
<li><p>device(type=’cuda’)         # Current CUDA device</p></li>
<li><p>device(type=’cuda’, index=0) # Specific CUDA device</p></li>
<li><p>device(type=’cuda:1’)       # CUDA device 1</p></li>
</ul>
</li>
<li><p>MPS (Metal Performance Shaders): For Apple Silicon GPUs on macOS.
Example: device(type=’mps’)</p></li>
<li><p>XLA: Used with PyTorch/XLA for TPU acceleration.
Example: device(type=’xla’)</p></li>
<li><p>IPU: For Graphcore Intelligence Processing Units.
Example: device(type=’ipu’)</p></li>
<li><p>XPU: For Intel GPUs.
Example: device(type=’xpu’)</p></li>
<li><p>Meta: A special device type for operations not requiring actual computation.
Example: device(type=’meta’)</p></li>
</ol>
<p>Note: CPU and CUDA are the most common device types. The availability of other types
depends on your hardware and PyTorch installation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cpu&#39;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="creating-our-own-neural-network">
<h2>Creating our own Neural Network<a class="headerlink" href="#creating-our-own-neural-network" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p><img alt="creating-neural-network-1" src="../_images/creating-neural-network-1.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create input_tensor with three features</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.342</span><span class="p">,</span> <span class="mf">0.543</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4533</span><span class="p">]]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A linear layer takes an input, applied a linear function, and returns output.</p>
<p>What happens in a linear layer is ‘For input ‘X’, weights ‘W0’ and bias ‘b0’, the linear layer performs</p>
<p>y0 = W0*X + b0</p>
<p>In PyTorch: Output = W0 &#64; input + b0</p>
<p>So when we call the nn.linear function below, it calls the Weights and Biases randomly so they are not yet useful. But we tune them in models so the output is meaningful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our first linear layer. in_features is the number of input layers and out_features is the number of output layers. Look at the image under &#39;Creating our own Neural Network&#39;.</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pass input through linear layer</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.0084,  0.1069]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<section id="getting-to-know-the-linear-layer-operation">
<h3>Getting to know the linear layer operation<a class="headerlink" href="#getting-to-know-the-linear-layer-operation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Each linear layer has a .weight and .bias</p></li>
<li><p>Each linear layer multiples its respective input with layer weights and adds biases</p></li>
<li><p>Even with multiple stacked linear layers, output still has linear relationship with input.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[-0.5330,  0.3063,  0.2679],
        [ 0.3688,  0.0437, -0.2508]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_layer</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([ 0.1290, -0.1566], requires_grad=True)
</pre></div>
</div>
</div>
</div>
</section>
<section id="lets-visualize-the-linear-layers-with-their-weights-and-biases">
<h3>Lets visualize the Linear_layers with their weights and biases<a class="headerlink" href="#lets-visualize-the-linear-layers-with-their-weights-and-biases" title="Link to this heading">#</a></h3>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ignore-the-code-block-below-it-is-just-for-visualization">
<h1>Ignore the code block below, it is just for visualization<a class="headerlink" href="#ignore-the-code-block-below-it-is-just-for-visualization" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="k">def</span> <span class="nf">visualize_linear_layer</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">):</span>
    <span class="c1"># Create a directed graph</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>

    <span class="c1"># Add input nodes</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

    <span class="c1"># Add output nodes</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

    <span class="c1"># Add edges with weights</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Output </span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># Set up the plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>

    <span class="c1"># Draw the graph</span>
    <span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> 
            <span class="n">node_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">font_weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

    <span class="c1"># Add edge labels (weights) with adjusted positions</span>
    <span class="n">edge_labels</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_edge_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edge_labels</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="n">edge_labels</span><span class="p">,</span> <span class="n">label_pos</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Add bias labels</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bias</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Bias: </span><span class="si">{</span><span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
                 <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear Layer Visualization&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Visualize the linear layer</span>
<span class="n">visualize_linear_layer</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

<span class="c1"># Print the output tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/wm/znp8c4pd0r72qm3gq7rpvwn40000gn/T/ipykernel_1075/4206505998.py:41: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
</pre></div>
</div>
<img alt="../_images/c5e118c13d4dbf8bc21fa9feda8e6b7b8baf4f0909a6fbe0bca35ea8c1dd9628.png" src="../_images/c5e118c13d4dbf8bc21fa9feda8e6b7b8baf4f0909a6fbe0bca35ea8c1dd9628.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output tensor:
tensor([[-0.0084,  0.1069]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Networks with only linear layers are called <strong>fully connected layers</strong>. Each neuron in one layer is connected to each neuron in the next layer.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="each-nn-linear-line-of-code-is-a-linear-layer">
<h1>Each <strong>nn.Linear</strong> line of code is a Linear Layer<a class="headerlink" href="#each-nn-linear-line-of-code-is-a-linear-layer" title="Link to this heading">#</a></h1>
<p>Here is a visual</p>
<p><img alt="neural-network-linear-layers" src="../_images/neural-network-linear-layers.png" /></p>
<section id="stacking-layers-with-nn-sequential">
<h2>Stacking layers with nn.Sequential()<a class="headerlink" href="#stacking-layers-with-nn-sequential" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A PyTorch container that allows us to stack multiple neural network modules in sequence.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create network with three linear layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=10, out_features=18, bias=True)
  (1): Linear(in_features=18, out_features=20, bias=True)
  (2): Linear(in_features=20, out_features=5, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create input_tensor with ten features</span>
<span class="n">input_tensor_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.342</span><span class="p">,</span> <span class="mf">0.543</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4533</span><span class="p">,</span> <span class="mf">0.434</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3435</span><span class="p">,</span> <span class="mf">0.3523</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3424</span><span class="p">,</span> <span class="mf">0.3453</span><span class="p">,</span> <span class="mf">0.87509</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3425</span><span class="p">]]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;This is a 1x10 dimentions tensor&#39;</span><span class="p">,</span> <span class="n">input_tensor_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This is a 1x10 dimentions tensor tensor([[ 0.3420,  0.5430, -0.4533,  0.4340, -0.3435,  0.3523, -0.3424,  0.3453,
          0.8751, -0.3425]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;here we received an output of 1x5 dimensions&#39;</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>here we received an output of 1x5 dimensions tensor([[-0.0547,  0.1139,  0.1321,  0.0467, -0.0807]],
       grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-1">
<h2>Exercise #1<a class="headerlink" href="#exercise-1" title="Link to this heading">#</a></h2>
<section id="your-first-neural-network">
<h3>Your first neural network<a class="headerlink" href="#your-first-neural-network" title="Link to this heading">#</a></h3>
<p>In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor.</p>
<p>The torch package and the torch.nn package have already been imported for you.</p>
<p>Create a neural network of two linear layers that takes a tensor of dimensions 1x8 as input, representing 8 features, and outputs a tensor of dimensions 1x1
Use any output dimension for the first layer you want.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Implement a small neural network with exactly two linear layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>  <span class="c1"># First linear layer with an intermediate output dimension</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># Second linear layer to produce the final output</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.1825]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="stacking-linear-layers">
<h3>Stacking linear layers<a class="headerlink" href="#stacking-linear-layers" title="Link to this heading">#</a></h3>
<p>Nice work building your first network with two linear layers. Let’s stack some more layers. Remember that a neural network can have as many hidden layers as we want, provided the inputs and outputs line up.</p>
<p>The aim of this exercise is for you to become comfortable thinking about the inputs and outputs of each successive layer in a PyTorch neural network.</p>
<p>This network is designed to ingest the following input:</p>
<p><code class="docutils literal notranslate"><span class="pre">input_tensor</span> <span class="pre">=</span> <span class="pre">torch.Tensor([[2,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">9,</span> <span class="pre">3,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">5,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">9]])</span></code></p>
<ul class="simple">
<li><p>Instructions: Reorder the items provided to create a neural network with three hidden layers and an output of size 2.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Explanation for the code below:</span>
<span class="c1"># 1. It takes an input tensor with 12 elements, as specified in the question.</span>
<span class="c1"># 2. It has three hidden layers:</span>
<span class="c1">#    - The first layer transforms from 12 to 20 dimensions</span>
<span class="c1">#    - The second layer transforms from 20 to 14 dimensions</span>
<span class="c1">#    - The third layer transforms from 14 to 3 dimensions</span>
<span class="c1"># 3. The final layer outputs a tensor of size 2, as required.</span>

<span class="c1"># The nn.Sequential module is used to stack these layers in order. Each nn.Linear layer performs a linear transformation on its input.</span>
<span class="c1"># The dimensions of each layer are chosen arbitrarily, but ensure that the output of one layer matches the input of the next.</span>
<span class="c1"># This structure allows the network to learn increasingly complex representations of the input data as it passes through each layer.</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>  <span class="c1"># First hidden layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>   <span class="c1"># Second hidden layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>    <span class="c1"># Third hidden layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>     <span class="c1"># Fourth the output layer</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.0975,  1.4123]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="there-are-multiple-types-of-layers">
<h2>There are multiple types of Layers<a class="headerlink" href="#there-are-multiple-types-of-layers" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Linear (Fully Connected) Layers</p></li>
<li><p>Convolutional Layers</p></li>
<li><p>Recurrent Layers (LSTM, GRU, Simple RNN)</p></li>
<li><p>Pooling Layers (Max, Average, Global)</p></li>
<li><p>Normalization Layers (Batch, Layer, Instance)</p></li>
<li><p>Activation Layers (ReLU, Sigmoid, Tanh, Softmax)</p></li>
<li><p>Dropout Layers</p></li>
<li><p>Flatten Layers</p></li>
<li><p>Embedding Layers</p></li>
<li><p>Attention Layers</p></li>
<li><p>Transformer Layers</p></li>
<li><p>Residual Connections</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="activation-functions">
<h1>Activation Functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h1>
<p><strong>Activation Functions</strong> add <strong>non-linearity</strong> to the network</p>
<section id="why-do-we-need-activation-functions">
<h2>Why do we need activation functions?<a class="headerlink" href="#why-do-we-need-activation-functions" title="Link to this heading">#</a></h2>
<p>Imagine a neural network as a big house with many rooms, and each neuron is like a light bulb in those rooms. Now, activation functions are like light switches for these bulbs.
Without switches (activation functions), the lights would always be on or off at the same brightness. This is boring and not very useful!
With switches (activation functions), we can control which lights are on, how bright they are, or even have dimmer switches for more control.
This control allows our neural network “house” to create complex lighting patterns, just like how real neural networks can learn complex patterns in data.</p>
<p><img alt="Activation-layers" src="../_images/activation-layers.png" /></p>
</section>
<section id="sigmoid-function-example-with-an-activation-layer">
<h2>Sigmoid function example with an Activation layer<a class="headerlink" href="#sigmoid-function-example-with-an-activation-layer" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We use Sigmoid for binary classification.</p></li>
<li><p>Sigmoid is used as the last step in network of linear layers is <strong>equivalent</strong> to tradtional logistic regression.</p></li>
</ul>
<p><img alt="sigmoid-function-example" src="../_images/sigmoid-function-example.png" /></p>
<p>Let me explain the limitations of sigmoid and softmax functions in a way that’s easy to understand.</p>
</section>
<section id="sigmoid-function-limitations">
<h2>Sigmoid Function Limitations<a class="headerlink" href="#sigmoid-function-limitations" title="Link to this heading">#</a></h2>
<p><strong>Vanishing Gradient Problem</strong>
Think of the sigmoid function like squeezing a balloon into a small box that only has space between 0 and 1. When you push really hard (large positive or negative numbers):</p>
<ul class="simple">
<li><p>The output gets stuck near 0 or 1</p></li>
<li><p>The gradient becomes extremely small, almost zero</p></li>
<li><p>This makes learning very slow or impossible, like trying to move through thick mud</p></li>
</ul>
<p><strong>Not Zero-Centered</strong></p>
<ul class="simple">
<li><p>The sigmoid function’s outputs are always positive (between 0 and 1)</p></li>
<li><p>This creates a zigzag pattern during training, making it slower to converge</p></li>
<li><p>It’s like trying to walk straight while constantly being pulled to one side</p></li>
</ul>
<p><strong>Computational Cost</strong></p>
<ul class="simple">
<li><p>The sigmoid function uses exponentials which are expensive to calculate</p></li>
<li><p>This makes training slower compared to simpler functions like ReLU</p></li>
</ul>
</section>
<section id="when-to-use-each">
<h2>When to Use Each<a class="headerlink" href="#when-to-use-each" title="Link to this heading">#</a></h2>
<p><strong>Use Sigmoid When:</strong></p>
<ul class="simple">
<li><p>You’re doing binary classification</p></li>
<li><p>You need outputs between 0 and 1</p></li>
<li><p>You’re working with gates in LSTM/GRU</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.9975])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="c1"># First Linear Layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># Second Linear Layer</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c1"># Sigmoid Activation Function</span>
<span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=6, out_features=4, bias=True)
  (1): Linear(in_features=4, out_features=1, bias=True)
  (2): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<section id="softmax-as-an-activation-layer">
<h3>Softmax as an Activation Layer<a class="headerlink" href="#softmax-as-an-activation-layer" title="Link to this heading">#</a></h3>
<p><img alt="softmax-activation-function" src="../_images/softmax-activation-function.png" /></p>
</section>
</section>
<section id="softmax-function-limitations">
<h2>Softmax Function Limitations<a class="headerlink" href="#softmax-function-limitations" title="Link to this heading">#</a></h2>
<p><strong>Numerical Instability</strong></p>
<ul class="simple">
<li><p>When dealing with very large numbers, the exponentials can explode</p></li>
<li><p>When dealing with very small numbers, they can vanish to zero</p></li>
<li><p>Modern implementations solve this by subtracting the maximum value first</p></li>
</ul>
<p><strong>Mutual Exclusivity</strong></p>
<ul class="simple">
<li><p>Softmax assumes classes are mutually exclusive (only one can be true)</p></li>
<li><p>Not suitable for multi-label problems where multiple classes can be true simultaneously</p></li>
<li><p>Like trying to say a picture contains both a cat AND a dog</p></li>
</ul>
</section>
<section id="better-alternatives">
<h2>Better Alternatives<a class="headerlink" href="#better-alternatives" title="Link to this heading">#</a></h2>
<p><strong>For Hidden Layers:</strong></p>
<ul class="simple">
<li><p>ReLU is generally preferred over sigmoid</p></li>
<li><p>It’s faster and doesn’t suffer from vanishing gradients as much</p></li>
<li><p>However, ReLU can “die” if learning rates are too high</p></li>
</ul>
<p><strong>For Output Layer:</strong></p>
<ul class="simple">
<li><p>Binary classification: Sigmoid is still good</p></li>
<li><p>Multi-class classification: Softmax is appropriate</p></li>
<li><p>Modern variations like Swish can sometimes perform better</p></li>
</ul>
</section>
<section id="id1">
<h2>When to Use Each<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p><strong>Use Softmax When:</strong></p>
<ul class="simple">
<li><p>You’re doing multi-class classification</p></li>
<li><p>You need probability distributions</p></li>
<li><p>You want outputs that sum to 1</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Creating an input tensor</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">4.3</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">]])</span> <span class="c1"># 1. The outer brackets [ ] create a list, which represents a batch of inputs. The inner brackets [ ] represent a single sample within that batch. This structure creates a 2-dimensional tensor with the shape (1, 3), where: 1 is the batch size (number of samples) 3 is the number of features for each sample</span>

<span class="c1"># Apply softmax along the last dimension</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

<span class="n">output_tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.1392, 0.8420, 0.0188]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-1-6">
<h1>Exercise 1.6<a class="headerlink" href="#exercise-1-6" title="Link to this heading">#</a></h1>
<section id="the-sigmoid-and-softmax-functions">
<h2>The sigmoid and softmax functions<a class="headerlink" href="#the-sigmoid-and-softmax-functions" title="Link to this heading">#</a></h2>
<p>The sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multi-class classification problems. This exercise will familiarize you with creating and using both functions.</p>
<p>Let’s say that you have a neural network that returned the values contained in the <strong>score</strong> tensor as a pre-activation output. You will apply activation functions to this output.</p>
<section id="instructions-1-2">
<h3>Instructions 1/2<a class="headerlink" href="#instructions-1-2" title="Link to this heading">#</a></h3>
<p>Create a sigmoid function and apply it on input_tensor to generate a probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Create a sigmoid function and apply it on input_tensor</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">probability</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.6900]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="instruction-2-2">
<h3>Instruction 2/2<a class="headerlink" href="#instruction-2-2" title="Link to this heading">#</a></h3>
<p>Create a softmax function and apply it on input_tensor to generate a probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Create a sigmoid function and apply it on input_tensor</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="forward-propagation-and-backpropagation">
<h1>Forward Propagation and Backpropagation<a class="headerlink" href="#forward-propagation-and-backpropagation" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<section id="what-is-forward-propagation">
<h2>What is Forward Propagation?<a class="headerlink" href="#what-is-forward-propagation" title="Link to this heading">#</a></h2>
<p>Forward propagation is like sending a message through a game of telephone, but with math. Imagine you and your friends are playing a game where you pass a message from one person to another, but each person changes the message a little bit based on some rules.
In a neural network, forward propagation is the process of taking an input (like an image or a number) and passing it through all the layers of the network to get an output (like a prediction or classification).</p>
</section>
<section id="why-is-it-used-and-important">
<h2>Why is it Used and Important?<a class="headerlink" href="#why-is-it-used-and-important" title="Link to this heading">#</a></h2>
<p>Forward propagation is used to make predictions or classifications. It’s important because:
It’s how the network “thinks”: Just like how your brain processes information to make decisions, forward propagation is how a neural network processes data to make predictions.
It’s the first step in learning: Before a network can learn, it needs to make a guess. Forward propagation is that guess.
It helps us understand what the network is doing: By following forward propagation, we can see how the network arrives at its conclusion.</p>
</section>
<section id="an-analogy-for-forward-propagation">
<h2>An Analogy for Forward Propagation<a class="headerlink" href="#an-analogy-for-forward-propagation" title="Link to this heading">#</a></h2>
<p>Think of forward propagation like baking a cake:
You start with ingredients (input data).
You mix them in bowls (hidden layers) using recipes (weights and biases).
You apply heat or cold (activation functions) at various stages.
Finally, you get a cake (output) which might be great or might need improvement.
Forward propagation is just the baking process.</p>
</section>
<section id="what-is-backpropagation">
<h2>What is Backpropagation?<a class="headerlink" href="#what-is-backpropagation" title="Link to this heading">#</a></h2>
<p>Backpropagation is like playing a game of “Hot and Cold” with your neural network. Remember when you were a kid, and someone would hide an object, and you’d try to find it? They’d say “warmer” when you got closer and “colder” when you moved away. Backpropagation is similar, but instead of finding a hidden object, we’re trying to find the best weights and biases for our neural network.</p>
</section>
<section id="id2">
<h2>Why is it Used and Important?<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Backpropagation is the learning process of neural networks. It’s important because:
It teaches the network: Just like how you learn from your mistakes, backpropagation helps the network learn from its errors.
It improves predictions: By adjusting weights and biases, the network gets better at making accurate predictions.
It’s efficient: Backpropagation is a smart way to calculate how each weight and bias affects the final error.</p>
</section>
<section id="an-analogy-for-backpropagation">
<h2>An Analogy for Backpropagation<a class="headerlink" href="#an-analogy-for-backpropagation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>You bake a cake (forward propagation) and submit it to judges.</p></li>
<li><p>The judges taste it and give you a score (calculate error).</p></li>
<li><p>You then think back through your baking process (backward pass):</p></li>
<li><p>Was it too sweet? Maybe use less sugar next time.</p></li>
<li><p>Too dry? Perhaps adjust the baking time.</p></li>
<li><p>Not chocolatey enough? Increase the cocoa powder.</p></li>
<li><p>You make these small adjustments to your recipe (update weights and biases).</p></li>
<li><p>You bake again, hoping for a better cake (and a better score) next time.</p></li>
</ul>
<p>Forward-and-Backward-Pass</p>
<p><img alt="Forward-and-Backward-Pass" src="../_images/Forward-and-Backward-Pass.png" /></p>
</section>
<section id="understanding-the-differences-between-terms">
<h2>Understanding the Differences Between Terms<a class="headerlink" href="#understanding-the-differences-between-terms" title="Link to this heading">#</a></h2>
<p>Think of training a neural network like teaching someone to cook the perfect dish. Let’s clarify these commonly confused terms:</p>
<p><strong>Forward Pass vs Forward Propagation</strong></p>
<ul class="simple">
<li><p><strong>Forward Pass</strong>: A single trip through the network, like one attempt at cooking a dish</p></li>
<li><p><strong>Forward Propagation</strong>: The complete process and mathematical rules of moving forward through the network, like the entire cooking technique and recipe</p></li>
</ul>
<p><strong>Backward Pass vs Backpropagation</strong></p>
<ul class="simple">
<li><p><strong>Backward Pass</strong>: The specific phase of calculating errors backwards through the network layers, like analyzing what went wrong in each step of cooking</p></li>
<li><p><strong>Backpropagation</strong>: The complete learning algorithm that includes:</p>
<ol class="arabic simple">
<li><p>The backward pass (finding what went wrong)</p></li>
<li><p>Calculating gradients (figuring out how to fix each step)</p></li>
<li><p>Updating weights (adjusting the recipe)</p></li>
</ol>
</li>
</ul>
<p><strong>Simple Analogy to Remember the Difference</strong>:
Imagine learning to cook pasta:</p>
<ul class="simple">
<li><p>Forward propagation is like having the complete recipe and cooking technique</p></li>
<li><p>A forward pass is one attempt at cooking the pasta</p></li>
<li><p>A backward pass is analyzing what went wrong in each step</p></li>
<li><p>Backpropagation is the whole learning process: analyzing mistakes, figuring out improvements, and updating your recipe</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Term</p></th>
<th class="head"><p>What it Does</p></th>
<th class="head"><p>Cooking Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Forward Pass</p></td>
<td><p>One instance of moving through network</p></td>
<td><p>One attempt at cooking</p></td>
</tr>
<tr class="row-odd"><td><p>Forward Propagation</p></td>
<td><p>Complete process of moving through network</p></td>
<td><p>Complete cooking technique</p></td>
</tr>
<tr class="row-even"><td><p>Backward Pass</p></td>
<td><p>Error calculation phase</p></td>
<td><p>Analyzing what went wrong</p></td>
</tr>
<tr class="row-odd"><td><p>Backpropagation</p></td>
<td><p>Complete learning algorithm</p></td>
<td><p>Entire process of learning to cook better</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="classification-vs-regression-problems-in-deep-learning">
<h1>Classification vs Regression Problems in Deep Learning<a class="headerlink" href="#classification-vs-regression-problems-in-deep-learning" title="Link to this heading">#</a></h1>
<p>Below, you will learn how to perform binary classification, multi-class classification, and regression problems. You will learn how to deal with each of these types of problems.</p>
<p>Let me explain these concepts using simple, everyday analogies.</p>
<section id="regression-continuous-output">
<h2>Regression (Continuous Output)<a class="headerlink" href="#regression-continuous-output" title="Link to this heading">#</a></h2>
<p><strong>The Temperature Analogy</strong>
Think of regression like a thermometer. The temperature can be any value within a range:</p>
<ul class="simple">
<li><p>72.5°F</p></li>
<li><p>72.6°F</p></li>
<li><p>72.543°F</p></li>
<li><p>73.0°F</p></li>
</ul>
<p>It’s “continuous” because there are infinite possible values between any two numbers. You can always add more decimal places.</p>
<p><strong>Other Examples</strong></p>
<ul class="simple">
<li><p>House prices: $245,678.50</p></li>
<li><p>Height: 5.7 feet</p></li>
<li><p>Weight: 150.3 pounds</p></li>
</ul>
<p>In regression forward propagation, the network tries to predict these smooth, flowing numbers that can take any value.</p>
</section>
<section id="classification-discrete-output">
<h2>Classification (Discrete Output)<a class="headerlink" href="#classification-discrete-output" title="Link to this heading">#</a></h2>
<p><strong>The Light Switch Analogy</strong>
Think of binary classification like a light switch:</p>
<ul class="simple">
<li><p>ON (1)</p></li>
<li><p>OFF (0)</p></li>
</ul>
<p>There’s nothing in between - no “half-on” state.</p>
<p><strong>The Fruit Bowl Analogy</strong>
For multi-class classification, think of sorting fruits:</p>
<ul class="simple">
<li><p>Apple</p></li>
<li><p>Orange</p></li>
<li><p>Banana</p></li>
<li><p>Pear</p></li>
</ul>
<p>Each fruit is a distinct category. You can’t have something that’s 60% apple and 40% orange.</p>
</section>
<section id="key-differences-in-forward-pass">
<h2>Key Differences in Forward Pass<a class="headerlink" href="#key-differences-in-forward-pass" title="Link to this heading">#</a></h2>
<p><strong>Regression Network</strong>
Like a price estimator at a car dealership:</p>
<ul class="simple">
<li><p>Takes in features (mileage, year, condition)</p></li>
<li><p>Can output any price ($15,432.67)</p></li>
<li><p>No boundaries on the output value</p></li>
</ul>
<p><strong>Classification Network</strong>
Like a doctor diagnosing an illness:</p>
<ul class="simple">
<li><p>Takes in symptoms</p></li>
<li><p>Must choose from specific diagnoses</p></li>
<li><p>Can’t create new categories</p></li>
</ul>
</section>
<section id="simple-real-world-example">
<h2>Simple Real-World Example<a class="headerlink" href="#simple-real-world-example" title="Link to this heading">#</a></h2>
<p><strong>Regression</strong>
Predicting a child’s height:</p>
<ul class="simple">
<li><p>Input: Parent’s height, age, nutrition</p></li>
<li><p>Output could be any number: 4.3 feet, 4.31 feet, 4.32 feet</p></li>
</ul>
<p><strong>Classification</strong>
Predicting a child’s t-shirt size:</p>
<ul class="simple">
<li><p>Input: Same data</p></li>
<li><p>Output must be: Small, Medium, Large</p></li>
<li><p>Can’t output “Medium-and-a-half”</p></li>
</ul>
<p>The main difference is that regression can predict any number along a continuous spectrum, while classification must choose from pre-defined categories or classes.</p>
<p><img alt="classification-vs-regression" src="../_images/classification-vs-regression.png" /></p>
</section>
<section id="binary-classification-forward-propagation">
<h2>Binary Classification: Forward Propagation<a class="headerlink" href="#binary-classification-forward-propagation" title="Link to this heading">#</a></h2>
<section id="the-video-game-character-selection-analogy">
<h3>The Video Game Character Selection Analogy<a class="headerlink" href="#the-video-game-character-selection-analogy" title="Link to this heading">#</a></h3>
<p>Imagine you’re creating a video game where you need to classify characters as either “Heroes” (1) or “Villains” (0). This is binary classification - just two choices!</p>
<p>Think of forward propagation like a character creation factory with different stations:</p>
</section>
<section id="input-station-input-layer">
<h3>Input Station (Input Layer)<a class="headerlink" href="#input-station-input-layer" title="Link to this heading">#</a></h3>
<p>You start with character features like:</p>
<ul class="simple">
<li><p>Height (in inches)</p></li>
<li><p>Strength level (1-100)</p></li>
<li><p>Smile factor (1-10)</p></li>
<li><p>Cape color brightness (1-10)</p></li>
</ul>
</section>
<section id="magic-processing-stations-hidden-layers">
<h3>Magic Processing Stations (Hidden Layers)<a class="headerlink" href="#magic-processing-stations-hidden-layers" title="Link to this heading">#</a></h3>
<p>Each station has workers (neurons) who look at these features:</p>
<ul class="simple">
<li><p>Station 1 worker might say: “Hmm, tall + strong usually means hero!”</p></li>
<li><p>Station 2 worker thinks: “Bright cape + big smile often means hero!”</p></li>
<li><p>They multiply features by importance (weights) and add their bias (personal opinion)</p></li>
</ul>
</section>
<section id="final-decision-station-output-layer">
<h3>Final Decision Station (Output Layer)<a class="headerlink" href="#final-decision-station-output-layer" title="Link to this heading">#</a></h3>
<p>The chief worker:</p>
<ul class="simple">
<li><p>Combines all opinions</p></li>
<li><p>Uses a special calculator (sigmoid function) that gives a number between 0 and 1</p></li>
<li><p>If result &gt; 0.5, character is labeled “Hero”</p></li>
<li><p>If result &lt; 0.5, character is labeled “Villain”</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Create input data of shape 5x6</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.4823</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2314</span><span class="p">,</span>  <span class="mf">0.7651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5982</span><span class="p">,</span>  <span class="mf">0.1245</span><span class="p">,</span>  <span class="mf">0.8976</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.7634</span><span class="p">,</span>  <span class="mf">0.3217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9012</span><span class="p">,</span>  <span class="mf">0.5678</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span>  <span class="mf">0.6789</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8765</span><span class="p">,</span>  <span class="mf">0.4321</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6543</span><span class="p">,</span>  <span class="mf">0.9876</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3210</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.1111</span><span class="p">,</span>  <span class="mf">0.2222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3333</span><span class="p">,</span>  <span class="mf">0.4444</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5555</span><span class="p">,</span>  <span class="mf">0.6666</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.7890</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4567</span><span class="p">,</span>  <span class="mf">0.1234</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9876</span><span class="p">,</span>  <span class="mf">0.5432</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2109</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># create a binary classification model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;You have 5 predictions (one for each input row)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Each number is between 0 and 1 (because of sigmoid)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Numbers &gt; 0.5 typically suggest class 1&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Numbers &lt; 0.5 typically suggest class 0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=6, out_features=4, bias=True)
  (1): Linear(in_features=4, out_features=1, bias=True)
  (2): Sigmoid()
)
tensor([[0.5112],
        [0.3748],
        [0.5165],
        [0.4205],
        [0.5367]], grad_fn=&lt;SigmoidBackward0&gt;)
You have 5 predictions (one for each input row)
Each number is between 0 and 1 (because of sigmoid)
Numbers &gt; 0.5 typically suggest class 1
Numbers &lt; 0.5 typically suggest class 0
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="multi-class-classification-forward-pass">
<h2>Multi-class classification: forward pass<a class="headerlink" href="#multi-class-classification-forward-pass" title="Link to this heading">#</a></h2>
<p>Let’s understand Multiclass Classification Forward Pass using a fun school-themed analogy! 📚</p>
</section>
<section id="the-school-club-assignment-system">
<h2>The School Club Assignment System<a class="headerlink" href="#the-school-club-assignment-system" title="Link to this heading">#</a></h2>
<p>Imagine you’re a teacher with a system that helps assign students to different school clubs: Art 🎨, Music 🎵, Sports 🏃‍♂️, or Science 🔬</p>
<section id="the-assignment-process-forward-pass">
<h3>The Assignment Process (Forward Pass)<a class="headerlink" href="#the-assignment-process-forward-pass" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Student Information Collection</strong> (Input Layer)</p></li>
</ol>
<ul class="simple">
<li><p>You collect 6 pieces of information about each student:</p>
<ul>
<li><p>Creativity score</p></li>
<li><p>Athletic ability</p></li>
<li><p>Test scores</p></li>
<li><p>Musical talent</p></li>
<li><p>Problem-solving skills</p></li>
<li><p>Interest level</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>The Counselor Review</strong> (Hidden Layer)</p></li>
</ol>
<ul class="simple">
<li><p>School counselors look at these features</p></li>
<li><p>Each counselor combines the information differently</p></li>
<li><p>They pass their recommendations to the final committee</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Final Club Assignment</strong> (Output Layer)</p></li>
</ol>
<ul class="simple">
<li><p>Instead of just two choices (like binary classification), we now have 4 possible clubs</p></li>
<li><p>Each student gets 4 scores (one for each club)</p></li>
<li><p>The highest score determines the club assignment</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Create input data of shape 5x6</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.4823</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2314</span><span class="p">,</span>  <span class="mf">0.7651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5982</span><span class="p">,</span>  <span class="mf">0.1245</span><span class="p">,</span>  <span class="mf">0.8976</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.7634</span><span class="p">,</span>  <span class="mf">0.3217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9012</span><span class="p">,</span>  <span class="mf">0.5678</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span>  <span class="mf">0.6789</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8765</span><span class="p">,</span>  <span class="mf">0.4321</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6543</span><span class="p">,</span>  <span class="mf">0.9876</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3210</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.1111</span><span class="p">,</span>  <span class="mf">0.2222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3333</span><span class="p">,</span>  <span class="mf">0.4444</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5555</span><span class="p">,</span>  <span class="mf">0.6666</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.7890</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4567</span><span class="p">,</span>  <span class="mf">0.1234</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9876</span><span class="p">,</span>  <span class="mf">0.5432</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2109</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Specify model has three classes</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Create multiclass classification model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="c1"># First linear layer </span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">),</span> <span class="c1"># Second linear Layer </span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Softmax activation</span>
<span class="p">)</span>

<span class="c1"># Pass input data through model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span> <span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Each row here sums up to 1:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of each row:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Each row is a unique feature called a class like Row 1 = class 1 (mammal), row 2 = class 1 (mammal), row 3 = class 3 (reptile)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 3])
Each row here sums up to 1: tensor([[0.3354, 0.2995, 0.3651],
        [0.2742, 0.3000, 0.4257],
        [0.3483, 0.2766, 0.3751],
        [0.2777, 0.3089, 0.4134],
        [0.3954, 0.2564, 0.3482]], grad_fn=&lt;SoftmaxBackward0&gt;)
Sum of each row: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)
Each row is a unique feature called a class like Row 1 = class 1 (mammal), row 2 = class 1 (mammal), row 3 = class 3 (reptile)
</pre></div>
</div>
</div>
</div>
</section>
<section id="key-differences-of-multi-class-classification-from-binary-classification">
<h3>Key Differences of Multi-class classification from Binary Classification:<a class="headerlink" href="#key-differences-of-multi-class-classification-from-binary-classification" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Multiple Outputs</strong></p></li>
</ol>
<ul class="simple">
<li><p>Binary: One output (0 or 1)</p></li>
<li><p>Multiclass: Multiple outputs (one for each class)</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Softmax Instead of Sigmoid</strong></p></li>
</ol>
<ul class="simple">
<li><p>Binary: Uses Sigmoid (0 to 1)</p></li>
<li><p>Multiclass: Uses Softmax (probabilities that sum to 1)</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Final Decision</strong></p></li>
</ol>
<ul class="simple">
<li><p>Binary: Above/below 0.5</p></li>
<li><p>Multiclass: Highest probability wins</p></li>
</ul>
</section>
<section id="why-it-s-important">
<h3>Why It’s Important:<a class="headerlink" href="#why-it-s-important" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Real-world problems often have more than two choices</p></li>
<li><p>Each class gets a fair chance</p></li>
<li><p>We can see confidence levels for all possibilities</p></li>
<li><p>Helps make more nuanced decisions</p></li>
</ul>
<p>Remember: The “Forward Pass” is just the one-way trip from input (student info) to output (club probabilities), like a student walking through the school offices getting their club assignment! 🏫</p>
</section>
</section>
<section id="regression-forward-propogation">
<h2>Regression: Forward Propogation<a class="headerlink" href="#regression-forward-propogation" title="Link to this heading">#</a></h2>
<p>Below the code does not have an activation function, and the last layer’s last dimension returns an output with one feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Create input data of shape 5x6</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.4823</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2314</span><span class="p">,</span>  <span class="mf">0.7651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5982</span><span class="p">,</span>  <span class="mf">0.1245</span><span class="p">,</span>  <span class="mf">0.8976</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.7634</span><span class="p">,</span>  <span class="mf">0.3217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9012</span><span class="p">,</span>  <span class="mf">0.5678</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span>  <span class="mf">0.6789</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8765</span><span class="p">,</span>  <span class="mf">0.4321</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6543</span><span class="p">,</span>  <span class="mf">0.9876</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3210</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.1111</span><span class="p">,</span>  <span class="mf">0.2222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3333</span><span class="p">,</span>  <span class="mf">0.4444</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5555</span><span class="p">,</span>  <span class="mf">0.6666</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.7890</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4567</span><span class="p">,</span>  <span class="mf">0.1234</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9876</span><span class="p">,</span>  <span class="mf">0.5432</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2109</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Create regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="c1"># First linear layer </span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Second linear layer</span>
<span class="p">)</span>

<span class="c1"># Pass input data through model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Return output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.2004],
        [-0.4410],
        [ 0.2510],
        [-0.2000],
        [ 0.2259]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-1">
<h2>Exercise 2.1<a class="headerlink" href="#exercise-2-1" title="Link to this heading">#</a></h2>
<section id="building-a-binary-classifier-in-pytorch">
<h3>Building a binary classifier in PyTorch<a class="headerlink" href="#building-a-binary-classifier-in-pytorch" title="Link to this heading">#</a></h3>
<p>Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.</p>
<p>In this exercise, you’ll practice building this small network and interpreting the output of the classifier.</p>
<p>The torch package and the torch.nn package have already been imported for you.</p>
<p>Instruction-</p>
<ul class="simple">
<li><p>Create a neural network that takes a tensor of dimensions 1x8 as input, and returns an output of the correct shape for binary classification.</p></li>
<li><p>Pass the output of the linear layer to a sigmoid, which both takes in and return a single float.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Implement a small neural network for binary classification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
  <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
  <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.9916]], grad_fn=&lt;SigmoidBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2-2">
<h2>Exercise 2.2<a class="headerlink" href="#exercise-2-2" title="Link to this heading">#</a></h2>
<section id="from-regression-to-multi-class-classification">
<h3>From regression to multi-class classification<a class="headerlink" href="#from-regression-to-multi-class-classification" title="Link to this heading">#</a></h3>
<p>Recall that the models we have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.</p>
<p>In this exercise, you’ll start by building a model for regression, and then tweak the model to perform a multi-class classification.</p>
</section>
<section id="id3">
<h3>Instructions 1/2<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Create a neural network with exactly four linear layers, which takes the input tensor as input, and outputs a regression value, using any shapes you like for the hidden layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Implement a neural network with exactly four linear layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.1058]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualizing the output above. Ignore the code. Just look at the visual</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Visualize the model architecture</span>
<span class="k">def</span> <span class="nf">plot_model_architecture</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">out_features</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)]</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">i</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">size</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="n">size</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">-</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="c1"># Add layer size text</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="s1">Size: </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
                <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># Increased upper limit</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Neural Network Architecture&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_model_architecture</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Print the output of each layer</span>
<span class="k">def</span> <span class="nf">print_layer_outputs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer outputs:&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">input_tensor</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> output shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final output: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">print_layer_outputs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e38a88d53cf2c92a0a6294d4e37411409d5a0ddab9817adde8e4537598ad0ca0.png" src="../_images/e38a88d53cf2c92a0a6294d4e37411409d5a0ddab9817adde8e4537598ad0ca0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer outputs:
Layer 0 output shape: torch.Size([1, 4])
Layer 1 output shape: torch.Size([1, 5])
Layer 2 output shape: torch.Size([1, 9])
Layer 3 output shape: torch.Size([1, 1])
Final output: tensor([[-0.1058]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Explanation for the code above-</p>
<p>Why do we need ‘1’ at the end of the last linear layer?</p>
<p><img alt="Exercise2.2-instruction1" src="../_images/Exercise2.2-instruction1.png" /></p>
<p>Here’s why we use 1 as the output in the last layer?</p>
<p><em><strong>Purpose of Regression</strong></em>: In regression, we’re trying to predict a single continuous value. Think of it like predicting the price of a house or the temperature tomorrow.
Single Output: The last layer outputs just one number because that’s all we need for regression. It’s like having a single thermometer to measure temperature.
No Activation Function: Notice there’s no activation function after the last layer. In regression, we often want the raw output value, which can be any real number.</p>
</section>
<section id="instructions-2-2">
<h3>Instructions 2/2<a class="headerlink" href="#instructions-2-2" title="Link to this heading">#</a></h3>
<p>A similar neural network to the one you just built is provided, containing four linear layers; update this network to perform a multi-class classification with four outputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Implement a neural network with exactly four linear layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.1340, 0.6609, 0.0776, 0.1275]], grad_fn=&lt;SoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualizing the output above. Ignore the code. Just look at the visual</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Visualize the model architecture</span>
<span class="k">def</span> <span class="nf">plot_model_architecture</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">out_features</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)]</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">i</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">size</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="n">size</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">-</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="c1"># Add layer size text</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="s1">Size: </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
                <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># Increased upper limit</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Neural Network Architecture&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_model_architecture</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Print the output of each layer</span>
<span class="k">def</span> <span class="nf">print_layer_outputs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer outputs:&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">input_tensor</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> output shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final output: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">print_layer_outputs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0c5f4ffb3bf106dd7b51e23aa33f4dab5f7dbcade12bebfc88d2ace35efcd264.png" src="../_images/0c5f4ffb3bf106dd7b51e23aa33f4dab5f7dbcade12bebfc88d2ace35efcd264.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer outputs:
Layer 0 output shape: torch.Size([1, 4])
Layer 1 output shape: torch.Size([1, 5])
Layer 2 output shape: torch.Size([1, 9])
Layer 3 output shape: torch.Size([1, 4])
Layer 4 output shape: torch.Size([1, 4])
Final output: tensor([[0.1340, 0.6609, 0.0776, 0.1275]], grad_fn=&lt;SoftmaxBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="explanation-for-the-code-above">
<h3>Explanation for the code above<a class="headerlink" href="#explanation-for-the-code-above" title="Link to this heading">#</a></h3>
<p>For the second part, we’re adapting the model for multi-class classification.</p>
<p>Here’s why we change the last layer to output 4 values and add a Softmax:</p>
<p><img alt="Exercise2.2-instruction2" src="../_images/Exercise2.2-instruction2.png" /></p>
<ol class="arabic simple">
<li><p><strong>Purpose of Classification</strong>: In multi-class classification, we’re trying to categorize input into one of several classes. Imagine sorting fruits into apples, oranges, bananas, and pears.</p></li>
<li><p><strong>Multiple Outputs</strong>: The last layer now outputs 4 numbers because we have 4 possible classes. It’s like having 4 baskets, one for each type of fruit.</p></li>
<li><p><strong>Softmax Activation</strong>: We add a Softmax layer at the end. This turns the 4 numbers into probabilities that sum to 1. It’s like saying, “There’s a 70% chance this is an apple, 20% orange, 5% banana, and 5% pear.”</p></li>
<li><p><strong>Why 4 Outputs</strong>: The number of outputs in the last layer should match the number of classes in your problem. If you’re classifying among 4 types of fruit, you need 4 outputs.</p></li>
</ol>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="using-loss-functions-to-assess-model-predictions">
<h1>Using Loss Functions to assess model predictions<a class="headerlink" href="#using-loss-functions-to-assess-model-predictions" title="Link to this heading">#</a></h1>
<section id="what-is-a-loss-function">
<h2>What is a Loss Function?<a class="headerlink" href="#what-is-a-loss-function" title="Link to this heading">#</a></h2>
<p>Imagine you’re playing a game where you have to guess the price of different items. The loss function is like a scorekeeper that tells you how far off your guess was from the real price. The bigger the difference, the higher your “loss” or score.
In machine learning, the loss function does the same thing. It measures how wrong the model’s predictions are compared to the actual correct answers.</p>
</section>
<section id="why-do-we-need-loss-functions">
<h2>Why Do We Need Loss Functions?<a class="headerlink" href="#why-do-we-need-loss-functions" title="Link to this heading">#</a></h2>
<p>Loss functions are super important because they help the machine learn. Here’s how:</p>
<ol class="arabic simple">
<li><p>The machine makes a guess (prediction)</p></li>
<li><p>The loss function calculates how wrong the guess was</p></li>
<li><p>The machine uses this information to make better guesses next time</p></li>
<li><p>It takes in model prediction ŷ and ground truth y
It’s like learning to shoot basketball free throws. Each time you miss, you adjust your technique based on how far off you were.</p></li>
</ol>
</section>
<section id="types-of-loss-functions">
<h2>Types of Loss Functions<a class="headerlink" href="#types-of-loss-functions" title="Link to this heading">#</a></h2>
<p>There are different types of loss functions for different kinds of problems. Let’s look at two main categories:</p>
<section id="classification-loss-functions">
<h3>Classification Loss Functions<a class="headerlink" href="#classification-loss-functions" title="Link to this heading">#</a></h3>
<p>These are used when we’re trying to categorize things, like determining if an email is spam or not.</p>
<ul class="simple">
<li><p>Binary Cross-Entropy: This is used when there are only two categories (like spam or not spam). It measures how confident the model is in its correct predictions.</p></li>
<li><p>Categorical Cross-Entropy: This is used when there are more than two categories (like classifying animals into dogs, cats, birds, etc.).</p></li>
</ul>
</section>
<section id="regression-loss-functions">
<h3>Regression Loss Functions<a class="headerlink" href="#regression-loss-functions" title="Link to this heading">#</a></h3>
<p>These are used when we’re trying to predict a number, like the price of a house or the temperature tomorrow.</p>
<ul class="simple">
<li><p>Mean Squared Error (MSE): This is like measuring the straight-line distance between your guess and the correct answer, then squaring it. It’s good for most cases but can be sensitive to outliers (really wrong guesses).</p></li>
<li><p>Mean Absolute Error (MAE): This just measures the straight-line distance without squaring. It’s less sensitive to outliers.</p></li>
</ul>
</section>
<section id="loss-function-in-a-formula">
<h3>Loss Function in a formula<a class="headerlink" href="#loss-function-in-a-formula" title="Link to this heading">#</a></h3>
<p>loss = F(y, ŷ)</p>
<p>Here we take the y which is our ground truth and ŷ as the prediction value as inputs which returns a numerical loss value.</p>
</section>
</section>
<section id="one-hot-encoding">
<h2>One Hot encoding<a class="headerlink" href="#one-hot-encoding" title="Link to this heading">#</a></h2>
<p>One Hot encoding is a technique used to convert categorical data into a format that machine learning models can understand and use effectively.</p>
<section id="here-s-how-it-works">
<h3>Here’s how it works:<a class="headerlink" href="#here-s-how-it-works" title="Link to this heading">#</a></h3>
<p>Instead of using a single number to represent a category, we create a new column for each possible category.</p>
<p>For each data point, we put a 1 in the column that represents its category, and 0s in all the other columns.</p>
<p>Let’s use a simple example:</p>
<p>Imagine we have a dataset about fruits with three categories: apple, banana, and orange. Instead of assigning numbers like 1, 2, 3 to these fruits, we create three new columns:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Is_Apple</p></th>
<th class="head"><p>Is_Banana</p></th>
<th class="head"><p>Is_Orange</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<p>This way:</p>
<ul class="simple">
<li><p>An apple is represented as [1, 0, 0]</p></li>
<li><p>A banana is represented as [0, 1, 0]</p></li>
<li><p>An orange is represented as [0, 0, 1]</p></li>
</ul>
</section>
<section id="why-do-we-use-one-hot-encoding">
<h3>Why do we use One Hot encoding?<a class="headerlink" href="#why-do-we-use-one-hot-encoding" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>It prevents the model from assuming a natural order between categories. For example, if we used 1, 2, 3 for apple, banana, orange, the model might think bananas are “more than” apples and “less than” oranges, which isn’t true.</p></li>
<li><p>It allows the model to treat each category independently, which can lead to better predictions.</p></li>
<li><p>It’s compatible with many machine learning algorithms that expect numerical input.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1211</span><span class="p">,</span> <span class="mf">0.1059</span><span class="p">]])</span>

<span class="n">one_hot_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;This is our loss function value:&#39;</span><span class="p">,</span> <span class="n">criterion</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">one_hot_target</span><span class="o">.</span><span class="n">double</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This is our loss function value: tensor(0.8131, dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1211</span><span class="p">,</span> <span class="mf">0.1059</span><span class="p">]])</span>
<span class="n">one_hot_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">loss_value</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">one_hot_target</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;This is our loss function value:&#39;</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Let&#39;s understand what this value means:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;0.0 - 0.3: Excellent! Model is very confident and correct&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;0.3 - 0.5: Good! Model is learning well&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;0.5 - 1.0: Okay, but needs improvement&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1.0 - 2.0: Poor performance, model is struggling&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt; 2.0: Very poor, model might be making very confident wrong predictions&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">loss_value</span> <span class="o">&lt;</span> <span class="mf">0.3</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✨ Your current loss is EXCELLENT!&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">loss_value</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✨ Your current loss is GOOD!&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">loss_value</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✨ Your current loss is OKAY, but could be better&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">loss_value</span> <span class="o">&lt;</span> <span class="mf">2.0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✨ Your current loss needs significant improvement&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✨ Your current loss indicates serious problems&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This is our loss function value: tensor(0.8131, dtype=torch.float64)

Let&#39;s understand what this value means:
0.0 - 0.3: Excellent! Model is very confident and correct
0.3 - 0.5: Good! Model is learning well
0.5 - 1.0: Okay, but needs improvement
1.0 - 2.0: Poor performance, model is struggling
&gt; 2.0: Very poor, model might be making very confident wrong predictions

✨ Your current loss is OKAY, but could be better
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss-function-takes">
<h3>Loss function takes:<a class="headerlink" href="#loss-function-takes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>scores</strong>: Model predictions before the final softmax function</p></li>
<li><p><strong>one_hot_target</strong>: One-hot encoded ground truth label</p></li>
</ul>
</section>
<section id="loss-function-outputs">
<h3>Loss function outputs:<a class="headerlink" href="#loss-function-outputs" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>loss</strong>: A single float value</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="our-training-goal-is-to-minimize-this-loss-the-lower-the-loss-the-better-the-model">
<h1>✨ <strong>Our training goal is to minimize this loss!</strong> The lower the loss, the better the model! ✨<a class="headerlink" href="#our-training-goal-is-to-minimize-this-loss-the-lower-the-loss-the-better-the-model" title="Link to this heading">#</a></h1>
<section id="exercise-2-3">
<h2>Exercise 2.3<a class="headerlink" href="#exercise-2-3" title="Link to this heading">#</a></h2>
<section id="creating-one-hot-encoded-labels">
<h3>Creating one-hot encoded labels<a class="headerlink" href="#creating-one-hot-encoded-labels" title="Link to this heading">#</a></h3>
<p>One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones. In this exercise, you’ll create the one-hot encoded vector of the label y provided.</p>
<p>You’ll practice doing this manually, and then make your life easier by leveraging the help of PyTorch! Your dataset contains three classes.</p>
<p>NumPy is already imported as np, and torch.nn.functional as F. The torch package is also imported.</p>
<ul class="simple">
<li><p>Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.</p></li>
<li><p>Create a one-hot encoded vector of the ground truth label y using PyTorch.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Create the one-hot encoded vector using NumPy</span>
<span class="n">one_hot_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Create the one-hot encoded vector using PyTorch</span>
<span class="n">one_hot_pytorch</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">one_hot_numpy</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="n">one_hot_pytorch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 1 0] and tensor([0, 1, 0])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2-4">
<h2>Exercise 2.4<a class="headerlink" href="#exercise-2-4" title="Link to this heading">#</a></h2>
<section id="calculating-cross-entropy-loss">
<h3>Calculating cross entropy loss<a class="headerlink" href="#calculating-cross-entropy-loss" title="Link to this heading">#</a></h3>
<p>Cross entropy loss is the most used loss for classification problems. In this exercise, you will create inputs and calculate cross entropy loss in PyTorch. You are provided with the ground truth label y and a vector of scores predicted by your model.</p>
<p>You’ll start by creating a one-hot encoded vector of the ground truth label y, which is a required step to compare y with the scores predicted by your model. Next, you’ll create a cross entropy loss function. Last, you’ll call the loss function, which takes scores (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample.</p>
</section>
<section id="instruction-1-3">
<h3>Instruction 1/3<a class="headerlink" href="#instruction-1-3" title="Link to this heading">#</a></h3>
<p>Create the one-hot encoded vector of the ground truth label y and assign it to one_hot_label.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]])</span>

<span class="c1"># Create a one-hot encoded vector of the label y</span>
<span class="n">one_hot_label</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">one_hot_label</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 0, 1, 0]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="instruction-2-3">
<h3>Instruction 2/3<a class="headerlink" href="#instruction-2-3" title="Link to this heading">#</a></h3>
<p>Create the cross entropy loss function and store it as criterion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]])</span>

<span class="c1"># Create a one-hot encoded vector of the label y</span>
<span class="n">one_hot_label</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create the cross entropy loss function</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="instruction-3-3">
<h3>Instruction 3/3<a class="headerlink" href="#instruction-3-3" title="Link to this heading">#</a></h3>
<p>Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]])</span>

<span class="c1"># Create a one-hot encoded vector of the label y</span>
<span class="n">one_hot_label</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create the cross entropy loss function</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Calculate the cross entropy loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">one_hot_label</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(8.0619, dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="using-derivatives-to-update-model-parameters-a-treasure-hunt-analogy">
<h1>Using Derivatives to Update Model Parameters: A Treasure Hunt Analogy<a class="headerlink" href="#using-derivatives-to-update-model-parameters-a-treasure-hunt-analogy" title="Link to this heading">#</a></h1>
<p><img alt="derivatives_analogy" src="../_images/derivatives_analogy.png" /></p>
<blockquote>
<div><p>Note: Gradient is often used interchangeably with the term “Derivative” in machine learning contexts.</p>
</div></blockquote>
<section id="the-treasure-hunt-game-an-analogy-for-gradient-descent">
<h2>The Treasure Hunt Game: An Analogy for Gradient Descent<a class="headerlink" href="#the-treasure-hunt-game-an-analogy-for-gradient-descent" title="Link to this heading">#</a></h2>
<p>Imagine you’re playing a unique video game where you must find a hidden treasure in a vast, hilly park while blindfolded. This game serves as an excellent analogy for how we train machine learning models using gradients.</p>
<section id="game-rules">
<h3>Game Rules:<a class="headerlink" href="#game-rules" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Start at a random location in the park.</p></li>
<li><p>Feel the ground around you to determine the downhill direction.</p></li>
<li><p>Take a small step in that downhill direction.</p></li>
<li><p>Repeat steps 2 and 3 until you can’t go any lower.</p></li>
</ol>
<p>This process mirrors gradient descent in machine learning!</p>
</section>
</section>
<section id="mapping-the-game-to-machine-learning-concepts">
<h2>Mapping the Game to Machine Learning Concepts<a class="headerlink" href="#mapping-the-game-to-machine-learning-concepts" title="Link to this heading">#</a></h2>
<section id="the-park-the-loss-landscape">
<h3>The Park = The “Loss Landscape”<a class="headerlink" href="#the-park-the-loss-landscape" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In machine learning, we have a “loss landscape” instead of a physical park.</p></li>
<li><p>The lowest point in this landscape represents the optimal model performance.</p></li>
</ul>
</section>
<section id="you-the-learning-algorithm">
<h3>You = The Learning Algorithm<a class="headerlink" href="#you-the-learning-algorithm" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Just as you search for the treasure, the learning algorithm seeks the best way to make predictions.</p></li>
</ul>
</section>
<section id="feeling-the-ground-calculating-gradients">
<h3>Feeling the Ground = Calculating Gradients<a class="headerlink" href="#feeling-the-ground-calculating-gradients" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>When you feel the ground to determine the downhill direction, it’s akin to calculating gradients in machine learning.</p></li>
<li><p>Gradients indicate which direction will most improve our model.</p></li>
</ul>
</section>
<section id="taking-steps-updating-the-model">
<h3>Taking Steps = Updating the Model<a class="headerlink" href="#taking-steps-updating-the-model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Each step in the game is like updating the model’s parameters (weights and biases).</p></li>
<li><p>We aim to incrementally improve the model with each update.</p></li>
</ul>
</section>
<section id="the-treasure-the-best-model">
<h3>The Treasure = The Best Model<a class="headerlink" href="#the-treasure-the-best-model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The treasure at the lowest point represents the optimal version of our model, where it makes the most accurate predictions.</p></li>
</ul>
</section>
</section>
<section id="important-concepts">
<h2>Important Concepts<a class="headerlink" href="#important-concepts" title="Link to this heading">#</a></h2>
<section id="learning-rate-step-size">
<h3>Learning Rate = Step Size<a class="headerlink" href="#learning-rate-step-size" title="Link to this heading">#</a></h3>
<p>In the game, the size of your steps is crucial:</p>
<ul class="simple">
<li><p>Tiny steps: You’ll eventually reach the goal, but it might take a very long time.</p></li>
<li><p>Huge leaps: You might overshoot and miss the lowest point entirely.</p></li>
</ul>
<blockquote>
<div><p>In machine learning, we call this the “learning rate.” It’s similar to choosing your character’s walking speed in the game.</p>
</div></blockquote>
</section>
<section id="local-minima-small-dips">
<h3>Local Minima = Small Dips<a class="headerlink" href="#local-minima-small-dips" title="Link to this heading">#</a></h3>
<p>Imagine small holes in the park that aren’t the true lowest point. In machine learning, we call these “local minima.” They can trick us into thinking we’ve found the optimal solution when we haven’t.</p>
</section>
<section id="convergence-finding-the-treasure">
<h3>Convergence = Finding the Treasure<a class="headerlink" href="#convergence-finding-the-treasure" title="Link to this heading">#</a></h3>
<p>When you finally reach the lowest point and find the treasure, that’s called “convergence” in machine learning. It signifies that we’ve found the best version of our model.</p>
</section>
</section>
<section id="why-this-is-fascinating">
<h2>Why this is Fascinating<a class="headerlink" href="#why-this-is-fascinating" title="Link to this heading">#</a></h2>
<p>Just as this game helps you find treasure without visual cues, gradient descent enables computers to learn complex tasks autonomously. Some impressive applications include:</p>
<ul class="simple">
<li><p>Facial recognition in photos</p></li>
<li><p>Language translation</p></li>
<li><p>Playing sophisticated games like chess</p></li>
</ul>
<p>The most remarkable aspect? The computer learns to perform these tasks independently, simply by taking small, calculated steps in the right direction, guided by gradients!</p>
<p><img alt="Connecting-derivatives-and-model-training" src="../_images/Connecting-derivatives-and-model-training.png" /></p>
</section>
<section id="below-is-an-example-of-backpropagation">
<h2>Below is an example of Backpropagation<a class="headerlink" href="#below-is-an-example-of-backpropagation" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>I’ll break down this code and explain what’s happening in a simple way, using analogies where helpful.</p>
</section>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>This code demonstrates a basic neural network that processes data through multiple layers and then learns from its mistakes using backpropagation. Think of it like a student solving a math problem, checking the answer, and then learning from their mistakes.</p>
</section>
<section id="code-breakdown">
<h2>Code Breakdown<a class="headerlink" href="#code-breakdown" title="Link to this heading">#</a></h2>
<section id="data-preparation">
<h3>1. Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.4823</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2314</span><span class="p">,</span> <span class="mf">0.7651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5982</span><span class="p">,</span> <span class="mf">0.1245</span><span class="p">,</span> <span class="mf">0.8976</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7634</span><span class="p">,</span> <span class="mf">0.3217</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.9012</span><span class="p">,</span> <span class="mf">0.5678</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span> <span class="mf">0.6789</span><span class="p">,</span> <span class="mf">0.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8765</span><span class="p">,</span> <span class="mf">0.4321</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6543</span><span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This creates our input data with 16 numbers</p></li>
<li><p>Think of these numbers as 16 different features about something we’re trying to predict</p></li>
<li><p>Like having 16 different pieces of information about a house (size, age, location, etc.) to predict its price</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Create a sample tensor with shape (1, 16) to match the input size of the first layer</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.4823</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2314</span><span class="p">,</span> <span class="mf">0.7651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5982</span><span class="p">,</span> <span class="mf">0.1245</span><span class="p">,</span> <span class="mf">0.8976</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7634</span><span class="p">,</span> <span class="mf">0.3217</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.9012</span><span class="p">,</span> <span class="mf">0.5678</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1543</span><span class="p">,</span> <span class="mf">0.6789</span><span class="p">,</span> <span class="mf">0.2345</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8765</span><span class="p">,</span> <span class="mf">0.4321</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6543</span><span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-architecture">
<h3>2. Model Architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This creates a neural network with three layers:</p>
<ul class="simple">
<li><p>First layer: Takes 16 inputs and produces 8 outputs</p></li>
<li><p>Second layer: Takes those 8 values and produces 4 outputs</p></li>
<li><p>Final layer: Takes 4 values and produces 2 outputs
It’s like a funnel that gradually processes and condenses information.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the model and run a forward pass</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-pass">
<h3>3. Forward Pass<a class="headerlink" href="#forward-pass" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This is where the data flows through our network</p></li>
<li><p>Like a assembly line where each station (layer) processes the information</p></li>
<li><p>The input goes through each layer, getting transformed until we get our final prediction</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loss-calculation-and-backpropagation">
<h3>4. Loss Calculation and Backpropagation<a class="headerlink" href="#loss-calculation-and-backpropagation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>This is where the learning happens:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">criterion</span> <span class="pre">=</span> <span class="pre">nn.CrossEntropyLoss()</span></code>: Defines how we measure mistakes</p>
<ul class="simple">
<li><p>Like having a scoring system to grade an answer</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">=</span> <span class="pre">torch.tensor()</span></code>: Sets the correct answer we’re aiming for</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">criterion(prediction,</span> <span class="pre">target)</span></code>: Calculates how wrong our prediction was</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>: This is the magical part where the network learns</p>
<ul class="simple">
<li><p>It’s like tracing back through your work to find where you made mistakes</p></li>
<li><p>The network adjusts its internal numbers (weights and biases) to do better next time</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the loss and compute the gradients</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Assuming a single target class for this example</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="accessing-gradients">
<h3>5. Accessing Gradients<a class="headerlink" href="#accessing-gradients" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
<span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
<span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<ul class="simple">
<li><p>These lines look at how much each part of the network needs to change</p></li>
<li><p>Think of it as a report card for each layer, showing where improvements are needed</p></li>
<li><p>The gradients tell us which direction and how much to adjust each parameter</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access each layer&#39;s gradients</span>
<span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
<span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
<span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[-0.0707, -0.0161,  0.2050, -0.0543],
         [ 0.0707,  0.0161, -0.2050,  0.0543]]),
 tensor([ 0.4731, -0.4731]))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="understanding-the-output-of-the-code-above">
<h2>Understanding the Output of the code above ⬆︎<a class="headerlink" href="#understanding-the-output-of-the-code-above" title="Link to this heading">#</a></h2>
<p>This output shows the gradients (suggested changes) for the weights and biases of one of the layers in your neural network. Let’s break it down:</p>
<section id="first-part-weight-gradients">
<h3>First Part (Weight Gradients)<a class="headerlink" href="#first-part-weight-gradients" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.1811</span><span class="p">,</span>  <span class="mf">0.2281</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2075</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0432</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1811</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2281</span><span class="p">,</span>  <span class="mf">0.2075</span><span class="p">,</span>  <span class="mf">0.0432</span><span class="p">]])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This is a 2×4 matrix of gradients for the weights</p></li>
<li><p>Each number represents how much and in which direction that particular weight should change</p></li>
<li><p>Positive numbers suggest increasing the weight</p></li>
<li><p>Negative numbers suggest decreasing the weight</p></li>
<li><p>Notice how the second row is exactly opposite to the first row (same numbers but opposite signs)</p></li>
</ul>
</section>
<section id="second-part-bias-gradients">
<h3>Second Part (Bias Gradients)<a class="headerlink" href="#second-part-bias-gradients" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.5457</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5457</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>These are the gradients for the bias terms</p></li>
<li><p>Again, they’re opposite numbers (one positive, one negative)</p></li>
<li><p>This suggests one bias needs to increase by 0.5457 while the other needs to decrease by the same amount</p></li>
</ul>
</section>
</section>
<section id="real-world-analogy">
<h2>Real-world Analogy<a class="headerlink" href="#real-world-analogy" title="Link to this heading">#</a></h2>
<p>Imagine you’re adjusting the controls of a radio:</p>
<ul class="simple">
<li><p>The weight gradients (first part) are like instructions for fine-tuning multiple knobs:</p>
<ul>
<li><p>“Turn this knob up by 0.1811”</p></li>
<li><p>“Turn that knob down by 0.2075”</p></li>
<li><p>etc.</p></li>
</ul>
</li>
<li><p>The bias gradients (second part) are like the main volume controls:</p>
<ul>
<li><p>“Turn the left speaker up by 0.5457”</p></li>
<li><p>“Turn the right speaker down by 0.5457”</p></li>
</ul>
</li>
</ul>
<p>The opposite signs in each row suggest this is likely the final layer of a binary classification problem, where the network is trying to decide between two categories. It’s like a see-saw - when one side goes up, the other must go down by the same amount to maintain balance.</p>
<p>These numbers are relatively small, which is good - it means the network is suggesting subtle adjustments rather than dramatic changes. This is typical of a network that’s learning properly.</p>
<p>Remember: These gradients are just suggestions for how to update the parameters. In actual training:</p>
<ol class="arabic simple">
<li><p>These values would be multiplied by a learning rate (usually a small number like 0.01)</p></li>
<li><p>The resulting values would then be used to update the actual weights and biases</p></li>
<li><p>This process would repeat many times with new data until the network learns effectively</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate is typically small</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Update the weights</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="n">weight_grad</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">weight_grad</span>

<span class="c1"># Update the biases</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
<span class="n">bias_grad</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">bias_grad</span>

<span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0566,  0.0739, -0.1505,  0.0997, -0.0820,  0.1738, -0.0234,  0.0563],
       grad_fn=&lt;SubBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p><img alt="convex-and-non-convex-functions" src="../_images/convex-and-non-convex-functions.png" /></p>
</section>
<section id="gradient-descent">
<h2>Gradient descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>• For non-convex functions, we will use an iterative process such as gradient descent
• In PyTorch, an optimizer takes care of weight updates
• The most common optimizer is stochastic gradient descent (SGD)</p>
<p>Let me explain the most important PyTorch optimizers and their use cases in simple terms.</p>
</section>
<section id="most-popular-pytorch-optimizers">
<h2>Most Popular PyTorch Optimizers<a class="headerlink" href="#most-popular-pytorch-optimizers" title="Link to this heading">#</a></h2>
<p><strong>1. Adam (Adaptive Moment Estimation)</strong></p>
<ul class="simple">
<li><p>The most widely used optimizer that combines the best properties of other optimizers</p></li>
<li><p>Best for:</p>
<ul>
<li><p>Deep neural networks</p></li>
<li><p>Noisy data</p></li>
<li><p>Large datasets</p></li>
<li><p>When you’re unsure which optimizer to use</p></li>
</ul>
</li>
</ul>
<p><strong>2. SGD (Stochastic Gradient Descent)</strong></p>
<ul class="simple">
<li><p>The classic, basic optimizer</p></li>
<li><p>Best for:</p>
<ul>
<li><p>Simple models</p></li>
<li><p>When you want predictable behavior</p></li>
<li><p>When you need to understand exactly how the model is learning</p></li>
</ul>
</li>
</ul>
<p><strong>3. AdamW</strong></p>
<ul class="simple">
<li><p>A variant of Adam that handles weight decay better</p></li>
<li><p>Best for:</p>
<ul>
<li><p>Large language models</p></li>
<li><p>Transfer learning</p></li>
<li><p>When regularization is important</p></li>
</ul>
</li>
</ul>
<p><strong>4. RMSprop</strong></p>
<ul class="simple">
<li><p>Adapts learning rates based on the magnitude of recent gradients</p></li>
<li><p>Best for:</p>
<ul>
<li><p>Recurrent Neural Networks (RNNs)</p></li>
<li><p>When dealing with non-stationary problems</p></li>
<li><p>Online learning scenarios</p></li>
</ul>
</li>
</ul>
<p><strong>5. Adagrad</strong></p>
<ul class="simple">
<li><p>Adapts learning rates for each parameter individually</p></li>
<li><p>Best for:</p>
<ul>
<li><p>Sparse data</p></li>
<li><p>Natural language processing tasks</p></li>
<li><p>When parameters need different learning rates</p></li>
</ul>
</li>
</ul>
</section>
<section id="how-to-use-optimizers">
<h2>How to Use Optimizers<a class="headerlink" href="#how-to-use-optimizers" title="Link to this heading">#</a></h2>
<p>Here’s a simple pattern for using any optimizer in PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">OptimizerName</span>

<span class="c1"># Create the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span> <span class="n">Name</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">other_parameters</span>
<span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Clear gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>       <span class="c1"># Compute gradients</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>      <span class="c1"># Update weights</span>
</pre></div>
</div>
</section>
<section id="choosing-the-right-optimizer">
<h2>Choosing the Right Optimizer<a class="headerlink" href="#choosing-the-right-optimizer" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Start with Adam</strong></p></li>
</ol>
<ul class="simple">
<li><p>It’s the most versatile and works well in most situations[2]</p></li>
<li><p>Good default learning rate: 0.001</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Try SGD with momentum</strong></p></li>
</ol>
<ul class="simple">
<li><p>If Adam isn’t giving good results</p></li>
<li><p>Often gives better final results but takes longer to train</p></li>
<li><p>Good default learning rate: 0.01</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Consider AdamW</strong></p></li>
</ol>
<ul class="simple">
<li><p>If you’re working with large models</p></li>
<li><p>When regularization is important</p></li>
<li><p>Good default learning rate: 0.001</p></li>
</ul>
</section>
<section id="tips-for-using-optimizers">
<h2>Tips for Using Optimizers<a class="headerlink" href="#tips-for-using-optimizers" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Learning Rate Matters</strong></p></li>
</ol>
<ul class="simple">
<li><p>Start with the recommended default learning rate for each optimizer</p></li>
<li><p>Use learning rate schedulers to adjust the rate during training</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Monitor Training</strong></p></li>
</ol>
<ul class="simple">
<li><p>Watch the loss curve</p></li>
<li><p>If loss is unstable: reduce learning rate</p></li>
<li><p>If loss decreases too slowly: increase learning rate</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Combine with Proper Initialization</strong></p></li>
</ol>
<ul class="simple">
<li><p>Good weight initialization helps optimizers work better</p></li>
<li><p>Consider using techniques like Xavier or Kaiming initialization</p></li>
</ul>
<p>Remember, the choice of optimizer can significantly impact your model’s performance, but it’s not the only factor. Good data preprocessing, model architecture, and hyperparameter tuning are equally important for successful training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="c1"># Create the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">optimizer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h2>
<section id="estimating-a-sample">
<h3>Estimating a sample<a class="headerlink" href="#estimating-a-sample" title="Link to this heading">#</a></h3>
<p>In previous exercises, you used linear layers to build networks.</p>
<p>Recall that the operation performed by nn.Linear() is to take an input
and apply the transformation
, where
and
are two tensors (called the weight and bias).</p>
<p>A critical part of training PyTorch models is to calculate gradients of the weight and bias tensors with respect to a loss function.</p>
<p>In this exercise, you will calculate weight and bias tensor gradients using cross entropy loss and a sample of data.</p>
<p>The following tensors are provided:</p>
<p>weight: a 2 x 9 -element tensor
bias: a 2 -element tensor
preds: a 1 x 2 -element tensor containing the model predictions
target: a 1 x 2 -element one-hot encoded tensor containing the ground-truth label</p>
</section>
<section id="instructions">
<h3>Instructions<a class="headerlink" href="#instructions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Use the criterion you have defined to calculate the loss value with respect to the predictions and target values.</p></li>
<li><p>Compute the gradients of the cross entropy loss.</p></li>
<li><p>Display the gradients of the weight and bias tensors, in that order.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Given tensors (as specified in the problem)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create preds using weight and bias to ensure it has a grad_fn</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>  <span class="c1"># Random input data</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="o">+</span> <span class="n">bias</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>  <span class="c1"># One-hot encoded ground-truth label</span>

<span class="c1"># Define the criterion (loss function)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Calculate the loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

<span class="c1"># Compute the gradients of the cross entropy loss</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Display the gradients of the weight and bias tensors, in that order</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weight gradients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bias gradients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weight gradients:
tensor([[-0.0279,  0.8420,  0.5403,  0.3917,  2.7796, -0.4396,  0.3951,  0.6509,
          0.1947],
        [ 0.0279, -0.8420, -0.5403, -0.3917, -2.7796,  0.4396, -0.3951, -0.6509,
         -0.1947]])

Bias gradients:
tensor([ 0.9026, -0.9026])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id4">
<h2>Exercise<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="accessing-the-model-parameters">
<h1>Accessing the model parameters<a class="headerlink" href="#accessing-the-model-parameters" title="Link to this heading">#</a></h1>
<p>A PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network. You won’t be accessing the sigmoid.</p>
<section id="id5">
<h2>Instructions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Access the weight parameter of the first linear layer.</p></li>
<li><p>Access the bias parameter of the second linear layer.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Access the weight of the first linear layer</span>
<span class="n">weight_0</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weight_0</span><span class="p">)</span>

<span class="c1"># Access the bias of the second linear layer</span>
<span class="n">bias_1</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bias_1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[ 0.1007,  0.1355, -0.2420,  0.2254, -0.1193,  0.1145, -0.2189, -0.1516,
          0.0128, -0.1910, -0.0069, -0.0492,  0.1097, -0.0799, -0.1503,  0.1503],
        [-0.2119,  0.1658,  0.2477,  0.0999, -0.1780, -0.1281, -0.0017,  0.0128,
          0.1156,  0.1133, -0.2437,  0.1407, -0.0016, -0.0720,  0.1981, -0.2247],
        [-0.1265,  0.1194,  0.1715, -0.1262, -0.2085,  0.1956,  0.0744,  0.2443,
          0.1639, -0.1672, -0.0905,  0.0255,  0.1939, -0.0213, -0.2377,  0.1402],
        [-0.1421,  0.0942,  0.1761,  0.1638,  0.1388,  0.1220, -0.0867,  0.0498,
         -0.1881, -0.0315, -0.2321, -0.0553, -0.1652,  0.0274,  0.2247, -0.1296],
        [-0.0181, -0.1084, -0.1627, -0.0601,  0.2452, -0.2261,  0.1423, -0.0453,
         -0.1460,  0.1429, -0.1736, -0.1298,  0.1720, -0.1818, -0.1450, -0.0904],
        [ 0.0860, -0.1848,  0.0547,  0.0688,  0.0832,  0.2113,  0.0006,  0.0357,
          0.1193, -0.0926, -0.1847,  0.0498, -0.0193, -0.2232,  0.2341, -0.1690],
        [ 0.1609, -0.1935,  0.0303, -0.1526,  0.0332, -0.2000, -0.1076,  0.1994,
          0.0983,  0.1202, -0.2349, -0.0678,  0.0121,  0.1278, -0.1585,  0.0367],
        [ 0.0469,  0.1542,  0.1159, -0.0802, -0.1117, -0.2207, -0.1406,  0.1643,
          0.0759, -0.0848,  0.1901,  0.0302,  0.0200,  0.2373, -0.1199,  0.0275]],
       requires_grad=True)
Parameter containing:
tensor([-0.0096, -0.2575], requires_grad=True)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h2>Exercise<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<section id="updating-the-weights-manually">
<h3>Updating the weights manually<a class="headerlink" href="#updating-the-weights-manually" title="Link to this heading">#</a></h3>
<p>Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. PyTorch functions can do what you’re about to do, but it’s helpful to do the work manually at least once, to understand what’s going on under the hood.</p>
<p>A neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update.</p>
<p>Instructions (1/2)</p>
<ol class="arabic simple">
<li><p>Create the gradient variables by accessing the local gradients of each weight tensor.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">weight0</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="n">weight1</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="n">weight2</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>

<span class="c1"># Access the gradients of the weight of each linear layer</span>
<span class="n">grads0</span> <span class="o">=</span> <span class="n">weight0</span><span class="o">.</span><span class="n">grad</span>
<span class="n">grads1</span> <span class="o">=</span> <span class="n">weight1</span><span class="o">.</span><span class="n">grad</span>
<span class="n">grads2</span> <span class="o">=</span> <span class="n">weight2</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p>Instructions (2/2)</p>
<ol class="arabic simple" start="2">
<li><p>Update the weights using the gradients scaled by the learning rate.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create your model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                     <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                     <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># 2. Create some sample data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># Example input</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Example target</span>

<span class="c1"># 3. Forward pass</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 4. Calculate loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># 5. Backward pass</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Now you can access the gradients!</span>
<span class="n">weight0</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="n">weight1</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="n">weight2</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>

<span class="n">grads0</span> <span class="o">=</span> <span class="n">weight0</span><span class="o">.</span><span class="n">grad</span>
<span class="n">grads1</span> <span class="o">=</span> <span class="n">weight1</span><span class="o">.</span><span class="n">grad</span>
<span class="n">grads2</span> <span class="o">=</span> <span class="n">weight2</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># Update weights</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">weight0</span> <span class="o">=</span> <span class="n">weight0</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grads0</span>
<span class="n">weight1</span> <span class="o">=</span> <span class="n">weight1</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grads1</span>
<span class="n">weight2</span> <span class="o">=</span> <span class="n">weight2</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grads2</span>

<span class="nb">print</span><span class="p">(</span><span class="n">weight0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weight1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weight2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.1756,  0.2143, -0.0752,  0.1730,  0.0428, -0.1495, -0.1446, -0.1847,
          0.0558,  0.2422, -0.1796, -0.1354, -0.1894, -0.2357, -0.1578,  0.2356],
        [-0.1300, -0.1680, -0.0539, -0.1575,  0.0416,  0.2216,  0.1542,  0.2387,
          0.2489,  0.1631,  0.0762, -0.1955,  0.1099, -0.1975, -0.1411, -0.1353],
        [-0.0021,  0.2051,  0.0892,  0.1231, -0.0434,  0.0682,  0.1105,  0.1450,
         -0.1151,  0.1737, -0.0711,  0.2478, -0.0625,  0.1525, -0.2411,  0.1311],
        [-0.0559, -0.2201, -0.0776, -0.1542, -0.1778,  0.1919,  0.2357,  0.0771,
         -0.1939, -0.0925, -0.0486,  0.2414, -0.2246,  0.2333, -0.1585, -0.1417],
        [ 0.0357,  0.1407,  0.1312, -0.1572, -0.0731,  0.1397, -0.0980, -0.0729,
         -0.2488, -0.1927,  0.0387, -0.1141, -0.1218, -0.2272, -0.0885,  0.0258],
        [ 0.1934, -0.0135,  0.0656,  0.0805, -0.0890,  0.0421, -0.0442, -0.2401,
         -0.1542, -0.0465, -0.1058, -0.1043,  0.1991,  0.0336, -0.0590,  0.1905],
        [ 0.2330, -0.2056, -0.0172, -0.0916, -0.0895,  0.1224,  0.2165,  0.2371,
         -0.0637,  0.1495, -0.0340,  0.1897, -0.1348,  0.0999, -0.1636, -0.0926],
        [ 0.0396, -0.1237,  0.1302, -0.1927, -0.0669,  0.1801,  0.0384, -0.1149,
         -0.2402,  0.1528,  0.1699,  0.2429,  0.0012,  0.2072,  0.0469, -0.0096]],
       grad_fn=&lt;SubBackward0&gt;)
tensor([[-0.3039,  0.1468,  0.0228,  0.2625,  0.2567,  0.3410, -0.1148, -0.2630],
        [ 0.3355,  0.2923, -0.1378,  0.2380, -0.1565,  0.1189,  0.0742,  0.2019],
        [-0.1422, -0.2386,  0.3158, -0.0527,  0.3268,  0.2594,  0.2852,  0.1418],
        [ 0.0894, -0.1186, -0.1934, -0.2009, -0.1337, -0.2555,  0.0485, -0.3390]],
       grad_fn=&lt;SubBackward0&gt;)
tensor([[-0.1347,  0.3989, -0.0684,  0.3997],
        [ 0.2599,  0.1738,  0.3817,  0.1524]], grad_fn=&lt;SubBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id7">
<h2>Exercise<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<section id="using-the-pytorch-optimizer">
<h3>Using the PyTorch optimizer<a class="headerlink" href="#using-the-pytorch-optimizer" title="Link to this heading">#</a></h3>
<p>In the previous exercise, you manually updated the weight of a network. You now know what’s going on under the hood, but this approach is not scalable to a network of many layers.</p>
<p>Thankfully, the PyTorch SGD optimizer does a similar job in a handful of lines of code. In this exercise, you will practice the last step to complete the training loop: updating the weights using a PyTorch optimizer.</p>
<p>A neural network has been created and provided as the model variable. This model was used to run a forward pass and create the tensor of predictions pred. The one-hot encoded tensor is named target and the cross entropy loss function is stored as criterion.</p>
<p>torch.optim as optim, and torch.nn as nn have already been loaded for you.</p>
<p>Instructions (1/2)</p>
<ol class="arabic simple">
<li><p>Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="c1"># Create the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">optimizer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
</pre></div>
</div>
</div>
</div>
<p>Instructions</p>
<ol class="arabic simple" start="2">
<li><p>Update the model’s parameters using the optimizer.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Create the optimizer</span>
<span class="c1"># optimizer = optim.SGD(model.parameters(), lr=0.001)</span>

<span class="c1"># loss = criterion(pred, target)</span>
<span class="c1"># loss.backward()</span>

<span class="c1"># # Update the model&#39;s parameters using the optimizer</span>
<span class="c1"># optimizer.step()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="writing-our-first-training-loop">
<h1>Writing our first Training Loop!<a class="headerlink" href="#writing-our-first-training-loop" title="Link to this heading">#</a></h1>
<section id="training-a-neural-network">
<h2>Training a neural network<a class="headerlink" href="#training-a-neural-network" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Create a model</p></li>
<li><p>Choose a loss function</p></li>
<li><p>Create a dataset</p></li>
<li><p>Define an optimizer</p></li>
<li><p>Run a training loop, where for each sample of the dataset, we repeat:</p>
<ol class="arabic simple">
<li><p>Calculating loss (forward pass)</p></li>
<li><p>Calculating local gradients</p></li>
<li><p>Updating model parameters</p></li>
</ol>
</li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introducting-the-mean-squared-error-loss">
<h1>Introducting the Mean Squared Error Loss<a class="headerlink" href="#introducting-the-mean-squared-error-loss" title="Link to this heading">#</a></h1>
<section id="understanding-mse-with-a-pizza-delivery-analogy">
<h2>Understanding MSE with a Pizza Delivery Analogy<a class="headerlink" href="#understanding-mse-with-a-pizza-delivery-analogy" title="Link to this heading">#</a></h2>
<p>Imagine you’re a pizza delivery person trying to get better at estimating delivery times. You tell customers how long their pizza will take, and you want to measure how good your predictions are.</p>
<p>Let’s say for 3 deliveries:</p>
<ul class="simple">
<li><p>You predicted 20 minutes, but it took 25 minutes</p></li>
<li><p>You predicted 30 minutes, but it took 28 minutes</p></li>
<li><p>You predicted 15 minutes, but it took 18 minutes</p></li>
</ul>
<p>MSE measures how “wrong” your predictions were by:</p>
<ol class="arabic simple">
<li><p>Finding the difference between prediction and actual time</p></li>
<li><p>Squaring these differences (to make negatives positive)</p></li>
<li><p>Taking the average</p></li>
</ol>
<p><strong>Why squaring?</strong> Think of it like this - if you’re 5 minutes late or 5 minutes early, customers are equally unhappy. Squaring makes both positive and negative errors count the same way!</p>
</section>
<section id="pytorch-implementation">
<h2>PyTorch Implementation<a class="headerlink" href="#pytorch-implementation" title="Link to this heading">#</a></h2>
<p>Here’s a simple example using PyTorch to calculate MSE for our pizza delivery predictions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Our pizza delivery predictions (in minutes)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">])</span>

<span class="c1"># Actual delivery times (in minutes)</span>
<span class="n">actual_times</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">25.0</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">18.0</span><span class="p">])</span>

<span class="c1"># Create MSE loss function</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Calculate the loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual_times</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;This means that on average the prediction were off by about </span><span class="si">{</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> minutes. (We calculated that by taking the square root of the Mean Squared Error value)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean Squared Error: 12.67
This means that on average the prediction were off by about 3.56 minutes. (We calculated that by taking the square root of the Mean Squared Error value)
</pre></div>
</div>
</div>
</div>
<p>In machine learning, we use this number to help our model improve. The model will try to make this number as small as possible, just like a pizza delivery person trying to get better at estimating delivery times!</p>
</section>
<section id="using-the-mseloss">
<h2>Using the MSELoss<a class="headerlink" href="#using-the-mseloss" title="Link to this heading">#</a></h2>
<p>For regression problems, we often use Mean Squared Error (MSE) as a loss function instead of cross-entropy. MSE calculates the squared difference between predicted values (y_pred) and actual values (y). In this exercise, you’ll compute MSE loss using both NumPy and PyTorch.</p>
<p>The torch package has been imported, along with numpy as np and torch.nn as nn.</p>
<ul class="simple">
<li><p>Calculate the MSELoss using NumPy.</p></li>
<li><p>Create a MSELoss function using PyTorch.</p></li>
<li><p>Convert y_pred and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Calculate the MSELoss using NumPy</span>
<span class="n">mse_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create the MSELoss function</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Calculate the MSELoss using the created loss function</span>
<span class="n">mse_pytorch</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mse_pytorch</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(81.)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="some-terminology-before-we-move-forward">
<h1>Some terminology before we move forward<a class="headerlink" href="#some-terminology-before-we-move-forward" title="Link to this heading">#</a></h1>
<section id="epoch">
<h2>Epoch<a class="headerlink" href="#epoch" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>If a variable called ‘num_epoch’ which defines the number of epochs = 3, this means we’ll go through our entire dataset 3 times</p></li>
<li><p>Each time through helps our model learn better, like how reading a chapter multiple times helps you understand it better</p></li>
<li><p>Think of an epoch like a full study session of a textbook. When you’re studying for a test, you might read through your textbook multiple times to really understand it. Each complete read-through is like one epoch.</p></li>
</ul>
<p>Imagine you’re reading that textbook, but instead of trying to memorize the whole book at once, you break it into smaller chunks (like chapters):</p>
</section>
<section id="batch-features">
<h2>batch_features<a class="headerlink" href="#batch-features" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>It is like getting a few math problems to solve</p></li>
</ul>
</section>
<section id="batch-labels">
<h2>batch_labels<a class="headerlink" href="#batch-labels" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>It is like having the answers to those problems</p></li>
</ul>
</section>
<section id="the-dataloader">
<h2>The dataloader<a class="headerlink" href="#the-dataloader" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>It is like your study guide that organizes these problems and answers into manageable chunks</p></li>
</ul>
</section>
<section id="writing-a-training-loop">
<h2>Writing a training loop<a class="headerlink" href="#writing-a-training-loop" title="Link to this heading">#</a></h2>
<p>In scikit-learn, the training loop is wrapped in the .fit() method, while in PyTorch, it’s set up manually. While this adds flexibility, it requires a custom implementation.</p>
<p>In this exercise, you’ll create a loop to train a model for salary prediction.</p>
<p>The show_results() function is provided to help you visualize some sample predictions.</p>
<p>The package imports provided are: pandas as pd, torch, torch.nn as nn, torch.optim as optim, as well as DataLoader and TensorDataset from torch.utils.data.</p>
<p>The following variables have been created: dataloader, containing the dataloader; model, containing the neural network; criterion, containing the loss function, nn.MSELoss(); optimizer, containing the SGD optimizer; and num_epochs, containing the number of epochs.</p>
<section id="instructions-1-3">
<h3>Instructions (1/3)<a class="headerlink" href="#instructions-1-3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.</p></li>
<li><p>Set the gradients of the optimizer to zero.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Loop over the number of epochs and then the dataloader</span>
<span class="c1"># for epoch in range(num_epochs):</span>
<span class="c1">#     for batch_features, bath_labels in dataloader:</span>
<span class="c1">#         #Set the gradient to zero</span>
<span class="c1">#         optimizer.zero_grad()</span>
</pre></div>
</div>
</div>
</div>
<p>The function -</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>This is like erasing your scratch paper before solving a new math problem:
In PyTorch, gradients accumulate by default (like keeping all your old work on the scratch paper)
We need to clear them before each new calculation
If we didn’t do this, it would be like trying to solve a new problem while still having all your old work cluttering your paper</p>
</section>
<section id="instructions-2-3">
<h3>Instructions (2/3)<a class="headerlink" href="#instructions-2-3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Write the forward pass.</p></li>
<li><p>Compute the MSE loss value using the criterion() function provided.</p></li>
<li><p>Compute the gradients.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Loop over the number of epochs and the dataloader</span>
<span class="c1"># for i in range(num_epochs):</span>
<span class="c1">#   for data in dataloader:</span>
<span class="c1">#     # Set the gradients to zero</span>
<span class="c1">#     optimizer.zero_grad()</span>
<span class="c1">#     # Run a forward pass</span>
<span class="c1">#     feature, target = data</span>
<span class="c1">#     prediction = model(feature)</span>
<span class="c1">#     # Calculate the loss</span>
<span class="c1">#     loss = criterion(prediction, target)</span>
<span class="c1">#     # Compute the gradients</span>
<span class="c1">#     loss.backward()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="instructions-3-3">
<h3>Instructions (3/3)<a class="headerlink" href="#instructions-3-3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Update the model’s parameters.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Loop over the number of epochs and the dataloader</span>
<span class="c1"># for i in range(num_epochs):</span>
<span class="c1">#   for data in dataloader:</span>
<span class="c1">#     # Set the gradients to zero</span>
<span class="c1">#     optimizer.zero_grad()</span>
<span class="c1">#     # Run a forward pass</span>
<span class="c1">#     feature, target = data</span>
<span class="c1">#     prediction = model(feature)    </span>
<span class="c1">#     # Calculate the loss</span>
<span class="c1">#     loss = criterion(prediction, target)    </span>
<span class="c1">#     # Compute the gradients</span>
<span class="c1">#     loss.backward()</span>
<span class="c1">#     # Update the model&#39;s parameters</span>
<span class="c1">#     optimizer.step()</span>
<span class="c1"># show_results(model, dataloader)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="section-3-neural-network-architecture-and-hyperparameters">
<h1>Section 3 - Neural Network Architecture and Hyperparameters<a class="headerlink" href="#section-3-neural-network-architecture-and-hyperparameters" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p>Hyperparameters are parameters, often chosen by the user, that control model training. The type of activation function, the number of layers in the model, and the learning rate are all hyperparameters of neural network training. Together, we will discover the most critical hyperparameters of a neural network and how to modify them.</p>
<section id="introducing-relu-rectified-linear-unit-and-leakyrelu">
<h2>Introducing ReLU (Rectified Linear Unit) and LeakyReLU<a class="headerlink" href="#introducing-relu-rectified-linear-unit-and-leakyrelu" title="Link to this heading">#</a></h2>
<ul>
<li><h3 class="rubric" id="these-2-are-also-activation-functions-like-sigmoid-and-softmax">These 2 are also Activation Functions like Sigmoid and SoftMax</h3>
</li>
</ul>
</section>
<section id="relu-rectified-linear-unit">
<h2>ReLU (Rectified Linear Unit)<a class="headerlink" href="#relu-rectified-linear-unit" title="Link to this heading">#</a></h2>
<p>Think of ReLU as a bouncer at a fancy nightclub who has one simple rule: “If you’re under 0, you’re not getting in!” Here’s how it works:</p>
<ul class="simple">
<li><p>If a number is positive, the bouncer lets it pass through unchanged</p></li>
<li><p>If a number is negative, the bouncer turns it into zero</p></li>
</ul>
<p>Imagine you’re playing with a light switch dimmer:</p>
<ul class="simple">
<li><p>When you push the dimmer up (positive values), the light gets brighter</p></li>
<li><p>But when you try to push it below zero, instead of getting “negative light,” the light just stays completely off</p></li>
</ul>
<p>This is exactly what ReLU does - it’s like a one-way street that only allows positive traffic.</p>
<p><strong>The Problem with ReLU</strong>
Here’s where it gets tricky. Sometimes, when ReLU keeps turning negative values to zero, it’s like giving someone the silent treatment - they can’t learn or improve because they’re getting no feedback at all. Scientists call this the “dying ReLU problem”.</p>
</section>
<section id="leaky-relu">
<h2>Leaky ReLU<a class="headerlink" href="#leaky-relu" title="Link to this heading">#</a></h2>
<p>Now, imagine instead of a strict bouncer, we have a more lenient one who says, “If you’re under 0, I’ll still let you in, but I’ll make you really tiny first.”</p>
<p>Think of Leaky ReLU like a leaky faucet:</p>
<ul class="simple">
<li><p>When it’s turned on (positive values), water flows normally</p></li>
<li><p>When it’s turned off (negative values), it still drips a tiny bit of water instead of being completely dry</p></li>
</ul>
<p>The “leak” is usually a small fraction (like 0.01) of the negative value that’s allowed to pass through. It’s like saying “Instead of completely ignoring negative values, let’s at least listen to them a tiny bit.”</p>
<p><strong>Why Leaky ReLU is Often Better</strong>
Going back to our nightclub analogy:</p>
<ul class="simple">
<li><p>Regular ReLU is like a bouncer who completely ignores rejected people</p></li>
<li><p>Leaky ReLU is like a bouncer who at least gives rejected people some feedback on why they were rejected, helping them improve for next time</p></li>
</ul>
<p>This small change helps prevent the “dying” problem because even negative values get a chance to contribute and learn, just at a much smaller scale.</p>
<p>Think of it like learning from mistakes:</p>
<ul class="simple">
<li><p>ReLU completely ignores mistakes (negative values)</p></li>
<li><p>Leaky ReLU learns a little bit even from mistakes, making it more resilient and adaptive</p></li>
</ul>
<p>This is why Leaky ReLU often performs better in deep neural networks - it keeps all neurons “alive” and learning, even if they make mistakes along the way.</p>
</section>
<section id="id8">
<h2>Exercise<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<section id="implementing-relu">
<h3>Implementing ReLU<a class="headerlink" href="#implementing-relu" title="Link to this heading">#</a></h3>
<p>The rectified linear unit (or ReLU) function is one of the most common activation functions in deep learning.</p>
<p>It overcomes the training problems linked with the sigmoid function you learned, such as the vanishing gradients problem.</p>
<p>In this exercise, you’ll begin with a ReLU implementation in PyTorch. Next, you’ll calculate the gradients of the function.</p>
<p>The nn module has already been imported for you.</p>
</section>
<section id="id9">
<h3>Instructions (1/2)<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create a ReLU function in PyTorch.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Create a ReLU function with PyTorch</span>
<span class="n">relu_pytorch</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id10">
<h3>Instructions (2/2)<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass.</p></li>
<li><p>Find the gradient at x.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Create a ReLU function with PyTorch</span>
<span class="n">relu_pytroch</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<span class="c1"># Apply the ReLU function on x, and calculate gradients</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">relu_pytorch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1">#Print the gradient of the ReLU function for x</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id11">
<h2>Exercise<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<section id="implementing-leaky-relu">
<h3>Implementing leaky ReLU<a class="headerlink" href="#implementing-leaky-relu" title="Link to this heading">#</a></h3>
<p>You’ve learned that ReLU is one of the most used activation functions in deep learning. You will find it in modern architecture. However, it does have the inconvenience of outputting null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training. Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs.</p>
<p>In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The numpy as np package, the torch package as well as the torch.nn as nn have already been imported.</p>
</section>
<section id="id12">
<h3>Instructions (1/2)<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create a leaky ReLU function in PyTorch with a negative slope of 0.05.</p></li>
<li><p>Call the function on the tensor x, which has already been defined for you.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Create a leaky relu function in PyTorch</span>
<span class="n">leaky_relu_pytorch</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># Call the above function on the tensor x</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">leaky_relu_pytorch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.1000)
</pre></div>
</div>
</div>
</div>
</section>
<section id="parameter-counting">
<h3>Parameter Counting<a class="headerlink" href="#parameter-counting" title="Link to this heading">#</a></h3>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="let-s-count-the-parameters-in-a-neural-network-using-a-simple-example">
<h1>Let’s count the parameters in a neural network using a simple example:<a class="headerlink" href="#let-s-count-the-parameters-in-a-neural-network-using-a-simple-example" title="Link to this heading">#</a></h1>
<section id="here-s-the-code-we-ll-analyze">
<h2>Here’s the code we’ll analyze:<a class="headerlink" href="#here-s-the-code-we-ll-analyze" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="the-restaurant-kitchen-model">
<h2>The Restaurant Kitchen Model<a class="headerlink" href="#the-restaurant-kitchen-model" title="Link to this heading">#</a></h2>
<p>Think of a neural network as a restaurant kitchen with multiple cooking stations (layers):</p>
<p><strong>First Layer (8 inputs → 4 stations)</strong></p>
<ul class="simple">
<li><p>You have 4 cooking stations (neurons)</p></li>
<li><p>Each station receives ingredients from 8 suppliers (inputs)</p></li>
<li><p>Each ingredient needs a measuring scale (weight)</p></li>
<li><p>Each station also has a master seasoning control (bias)</p></li>
<li><p>So each station needs: 8 measuring scales + 1 seasoning control = 9 parameters</p></li>
<li><p>Total for all stations: 4 stations × 9 parameters = 36 parameters</p></li>
</ul>
<p><strong>Second Layer (4 stations → 2 final plates)</strong></p>
<ul class="simple">
<li><p>You have 2 final plating stations (output neurons)</p></li>
<li><p>Each receives dishes from all 4 cooking stations</p></li>
<li><p>Each plating station needs 4 portion controls (weights)</p></li>
<li><p>Each also has a final taste adjuster (bias)</p></li>
<li><p>So each plating station needs: 4 portion controls + 1 taste adjuster = 5 parameters</p></li>
<li><p>Total for plating: 2 stations × 5 parameters = 10 parameters</p></li>
</ul>
<p><strong>Why This Matters</strong></p>
<ul class="simple">
<li><p>Just as a kitchen needs to know its equipment capacity, neural networks need to track their parameters</p></li>
<li><p>Too few parameters (like too few cooking tools) = limited menu options</p></li>
<li><p>Too many parameters (like too many unnecessary tools) = wasteful and harder to manage</p></li>
<li><p>The right number of parameters (like a well-equipped kitchen) = efficient operation and good results</p></li>
</ul>
<p>This kitchen setup, with its total of 46 parameters (cooking controls), represents the complete “recipe-making capability” of your neural network.</p>
<section id="id13">
<h3>Exercise<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>Counting the number of parameters
Deep learning models are famous for having a lot of parameters. Recent language models have billions of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.</p>
<p>In this exercise, you will calculate the number of parameters in your model, first manually, and then using PyTorch.</p>
</section>
<section id="id14">
<h3>Instructions (1/2)<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Calculate manually the number of parameters of the model below. How many does it have?</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The total number of parameters are&#39;</span><span class="p">,</span> <span class="p">((</span><span class="mi">16</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="p">((</span><span class="mi">4</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="p">((</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The total number of parameters are 81
</pre></div>
</div>
</div>
</div>
</section>
<section id="id15">
<h3>Instructions (2/2)<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Now, confirm your manual calculation by iterating through the model’s parameters to update the total variable with the total number of parameters in the model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Calculate the number of parameters in the model</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">total</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The number of parameters in the model is </span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The number of parameters in the model is 81
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-the-code-above-does">
<h3>What the code above does<a class="headerlink" href="#what-the-code-above-does" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="understanding-the-parameter-counter">
<h2>Understanding the Parameter Counter<a class="headerlink" href="#understanding-the-parameter-counter" title="Link to this heading">#</a></h2>
<p><strong>Setting Up the Counter</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>Think of <code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">0</span></code> like starting with an empty jar where you’ll collect marbles. Before you start counting your marbles, you need to make sure your jar is empty. That’s exactly what <code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">0</span></code> does - it creates our empty counting jar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">total</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Counting Loop (for p in model.parameters())</strong>
Now, imagine your neural network model is like having several bags of marbles:</p>
<ul class="simple">
<li><p>Each layer (Linear) is like a separate bag</p></li>
<li><p>Each bag contains different numbers of marbles (parameters)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">p.numel()</span></code> is like counting the marbles in each bag</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">+=</span> <span class="pre">p.numel()</span></code> is like taking the count from each bag and adding it to our jar</p></li>
</ul>
<p>Let’s break down what happens in each iteration:</p>
<ol class="arabic simple">
<li><p>The loop looks at one bag (layer) at a time</p></li>
<li><p>Counts all the marbles (parameters) in that bag</p></li>
<li><p>Adds that number to our running total in the jar</p></li>
</ol>
<p>This is important because in deep learning, we need to know how many parameters (or “marbles”) our model has to:</p>
<ul class="simple">
<li><p>Understand how complex our model is</p></li>
<li><p>Figure out how much memory it will need</p></li>
<li><p>Know how much computing power we’ll need to train it</p></li>
</ul>
<section id="id16">
<h3>Exercise<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
</section>
<section id="manipulating-the-capacity-of-a-network">
<h3>Manipulating the capacity of a network<a class="headerlink" href="#manipulating-the-capacity-of-a-network" title="Link to this heading">#</a></h3>
<p>In this exercise, you will practice creating neural networks with different capacities. The capacity of a network reflects the number of parameters in said network. To help you, a calculate_capacity() function has been implemented, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_capacity</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">total</span>
</pre></div>
</div>
<p>This function returns the number of parameters in your model.</p>
<p>The dataset you are training this network on has n_features features and n_classes classes. The torch.nn package has been imported as nn.</p>
</section>
<section id="id17">
<h3>Instructions (1/2)<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create a neural network with exactly three linear layers and less than 120 parameters, which takes n_features as inputs and outputs n_classes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Create a neural network with less than 120 parameters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Calculate the number of parameters in the model</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">total</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

<span class="n">total</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>118
</pre></div>
</div>
</div>
</div>
</section>
<section id="id18">
<h3>Instructions (2/2)<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create a neural network with exactly four linear layers and more than 120 parameters, which takes n_features as inputs and outputs n_classes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Create a neural network with less than 120 parameters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Calculate the number of parameters in the model</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">total</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

<span class="n">total</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>124
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="learning-rate-and-momentum-with-sgd-optimizer-stocahastic-gradient-descent">
<h2>Learning Rate and Momentum with SGD Optimizer (Stocahastic Gradient Descent)<a class="headerlink" href="#learning-rate-and-momentum-with-sgd-optimizer-stocahastic-gradient-descent" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>Let me explain Stochastic Gradient Descent (SGD) optimization using a fun skiing analogy that will tie everything together!</p>
</section>
<section id="the-skiing-adventure-of-sgd">
<h2>The Skiing Adventure of SGD<a class="headerlink" href="#the-skiing-adventure-of-sgd" title="Link to this heading">#</a></h2>
<p>Imagine you’re skiing down a mountain trying to reach the lowest point (which represents finding the minimum loss/error in our model). You’re not a professional skier, so you need to make decisions about how to get down safely and efficiently.</p>
</section>
<section id="learning-rate-your-skiing-speed">
<h2>Learning Rate: Your Skiing Speed<a class="headerlink" href="#learning-rate-your-skiing-speed" title="Link to this heading">#</a></h2>
<p>The learning rate is like how fast you decide to ski down the mountain. Think of it as your speed control:</p>
<p><strong>Too High Learning Rate (Skiing Too Fast)</strong></p>
<ul class="simple">
<li><p>Like zooming down at dangerous speeds</p></li>
<li><p>You might overshoot the bottom and crash</p></li>
<li><p>In ML terms: Your model might miss the optimal solution and bounce around, never finding the best weights</p></li>
<li><p>You might even “fly off the mountain” (model weights explode)
<img alt="Impact of the learning rate - high learning rate" src="../_images/Impact-of-the-learning-rate-high-learning-rate.png" /></p></li>
</ul>
<p><strong>Too Low Learning Rate (Skiing Too Slowly)</strong></p>
<ul class="simple">
<li><p>Like inching down the mountain at a snail’s pace</p></li>
<li><p>You’ll eventually get there, but it’ll take forever</p></li>
<li><p>In ML terms: Training will be very slow and might get stuck in small dips before reaching the bottom
<img alt="Impact of the learning rate - small learning rate" src="../_images/Impact-of-the-learning-rate-small-learning-rate.png" /></p></li>
</ul>
<p><strong>Just Right Learning Rate</strong></p>
<ul class="simple">
<li><p>Moving at a controlled, steady pace</p></li>
<li><p>Allows you to adjust your path while making good progress</p></li>
<li><p>Typical values often start around 0.01 or 0.001</p></li>
</ul>
<p>Let’s continue with our skiing analogy to understand momentum in more detail!</p>
</section>
<section id="understanding-momentum-the-skiing-inertia">
<h2>Understanding Momentum: The Skiing Inertia<a class="headerlink" href="#understanding-momentum-the-skiing-inertia" title="Link to this heading">#</a></h2>
<p>Imagine you’re wearing a heavy backpack while skiing. The weight of this backpack represents momentum - it affects how you move down the slope and how easily you can change direction.</p>
</section>
<section id="high-momentum-0-9-0-99">
<h2>High Momentum (0.9 - 0.99)<a class="headerlink" href="#high-momentum-0-9-0-99" title="Link to this heading">#</a></h2>
<p><strong>What It’s Like</strong></p>
<ul class="simple">
<li><p>Like skiing with a heavy backpack</p></li>
<li><p>Once you start moving in a direction, it’s harder to make sudden turns</p></li>
<li><p>You maintain more of your previous direction and speed</p></li>
</ul>
<p><img alt="high-momentum" src="../_images/high-momentum.png" /></p>
<p><strong>Benefits</strong></p>
<ul class="simple">
<li><p>Helps push through flat spots or small uphill sections</p></li>
<li><p>Like using your built-up speed to glide over small bumps</p></li>
<li><p>Great for escaping shallow valleys (local minima) in the loss landscape</p></li>
<li><p>Training tends to be more stable and faster overall</p></li>
</ul>
<p><strong>Potential Problems</strong></p>
<ul class="simple">
<li><p>Might overshoot the bottom of the slope</p></li>
<li><p>Harder to make precise adjustments</p></li>
<li><p>Like trying to stop quickly with a heavy backpack - it takes longer</p></li>
<li><p>Could miss the optimal solution if it requires sharp turns</p></li>
</ul>
</section>
<section id="low-momentum-0-1-0-5">
<h2>Low Momentum (0.1 - 0.5)<a class="headerlink" href="#low-momentum-0-1-0-5" title="Link to this heading">#</a></h2>
<p><strong>What It’s Like</strong></p>
<ul class="simple">
<li><p>Like skiing with a light backpack</p></li>
<li><p>Easier to make quick direction changes</p></li>
<li><p>Less influenced by your previous movement</p></li>
</ul>
<p><img alt="low-momentum" src="../_images/low-momentum.png" /></p>
<p><strong>Benefits</strong></p>
<ul class="simple">
<li><p>More precise control over your movement</p></li>
<li><p>Better for navigating tricky, winding paths</p></li>
<li><p>Useful when you need to make careful adjustments</p></li>
<li><p>Good for fine-tuning near the optimal solution</p></li>
</ul>
<p><strong>Potential Problems</strong></p>
<ul class="simple">
<li><p>Might get stuck in small dips</p></li>
<li><p>Progress can be slower</p></li>
<li><p>Like stopping at every little bump in the snow</p></li>
<li><p>More susceptible to getting trapped in local minima</p></li>
</ul>
</section>
<section id="zero-momentum-0-0">
<h2>Zero Momentum (0.0)<a class="headerlink" href="#zero-momentum-0-0" title="Link to this heading">#</a></h2>
<p><strong>What It’s Like</strong></p>
<ul class="simple">
<li><p>Like skiing with no backpack at all</p></li>
<li><p>Each move is independent of previous moves</p></li>
<li><p>Pure SGD without any memory of past updates</p></li>
</ul>
<p><strong>When to Use It</strong></p>
<ul class="simple">
<li><p>When you want very precise control</p></li>
<li><p>In simple landscapes with clear paths to the minimum</p></li>
<li><p>When dealing with noisy or unpredictable data</p></li>
</ul>
</section>
<section id="real-world-scenarios">
<h2>Real-World Scenarios<a class="headerlink" href="#real-world-scenarios" title="Link to this heading">#</a></h2>
<p><strong>High Momentum Works Best When</strong></p>
<ul class="simple">
<li><p>Your loss landscape is like a smooth, wide valley</p></li>
<li><p>You’re dealing with consistent patterns in your data</p></li>
<li><p>You want faster convergence</p></li>
<li><p>Like skiing down a long, gentle slope where you can use your momentum effectively</p></li>
</ul>
<p><strong>Low Momentum Works Best When</strong></p>
<ul class="simple">
<li><p>Your loss landscape is tricky with lots of turns</p></li>
<li><p>You’re near the optimal solution and need precision</p></li>
<li><p>Your data has lots of variation</p></li>
<li><p>Like skiing through a technical course where you need careful control</p></li>
</ul>
</section>
<section id="common-momentum-values-and-their-effects">
<h2>Common Momentum Values and Their Effects<a class="headerlink" href="#common-momentum-values-and-their-effects" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Momentum Value</p></th>
<th class="head"><p>Behavior</p></th>
<th class="head"><p>Best Used For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.9</p></td>
<td><p>Standard choice, good balance</p></td>
<td><p>Most training scenarios</p></td>
</tr>
<tr class="row-odd"><td><p>0.99</p></td>
<td><p>Very aggressive momentum</p></td>
<td><p>Large datasets, smooth loss landscapes</p></td>
</tr>
<tr class="row-even"><td><p>0.5</p></td>
<td><p>Moderate momentum</p></td>
<td><p>When high momentum is too aggressive</p></td>
</tr>
<tr class="row-odd"><td><p>0.0</p></td>
<td><p>No momentum</p></td>
<td><p>Precise control needed</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="practical-tips">
<h2>Practical Tips<a class="headerlink" href="#practical-tips" title="Link to this heading">#</a></h2>
<p><strong>Starting Out</strong></p>
<ul class="simple">
<li><p>Begin with momentum = 0.9</p></li>
<li><p>Like starting on a gentler slope before tackling steeper ones</p></li>
<li><p>Observe how your model behaves</p></li>
</ul>
<p><strong>Adjusting Momentum</strong></p>
<ul class="simple">
<li><p>If training is unstable: Lower the momentum</p></li>
<li><p>If training is too slow: Try increasing momentum</p></li>
<li><p>If close to convergence: Consider reducing momentum for fine-tuning</p></li>
</ul>
<p><strong>Warning Signs</strong></p>
<ul class="simple">
<li><p>If your loss is bouncing wildly: Your momentum might be too high</p></li>
<li><p>If progress is very slow: Your momentum might be too low</p></li>
<li><p>If you’re overshooting repeatedly: Consider reducing both momentum and learning rate</p></li>
</ul>
<p>Remember: Just like a skilled skier adjusts their technique based on the terrain, you’ll need to adjust momentum based on your model’s behavior and your training data’s characteristics. The goal is to find that sweet spot where you’re making steady progress while maintaining control!</p>
</section>
<section id="finding-the-right-balance">
<h2>Finding the Right Balance<a class="headerlink" href="#finding-the-right-balance" title="Link to this heading">#</a></h2>
<p>Just like a good skier needs to balance speed and control, you need to tune both parameters:</p>
<p><strong>Good Combination</strong></p>
<ul class="simple">
<li><p>Moderate learning rate with some momentum</p></li>
<li><p>Like skiing with controlled speed while maintaining smooth motion</p></li>
<li><p>Typically momentum values between 0.9 to 0.99 work well</p></li>
</ul>
<p><strong>Bad Combinations to Avoid</strong></p>
<ul class="simple">
<li><p>High learning rate + high momentum = Like skiing too fast with no ability to stop</p></li>
<li><p>Very low learning rate + no momentum = Like being stuck in deep snow</p></li>
<li><p>High momentum + very low learning rate = Like trying to push through molasses</p></li>
</ul>
</section>
<section id="real-world-impact">
<h2>Real-World Impact<a class="headerlink" href="#real-world-impact" title="Link to this heading">#</a></h2>
<p><strong>When It Works Well</strong></p>
<ul class="simple">
<li><p>Your model learns smoothly and consistently</p></li>
<li><p>Training progress shows steady improvement</p></li>
<li><p>Like a perfect ski run where you maintain control while making good progress</p></li>
</ul>
<p><strong>When It Goes Wrong</strong></p>
<ul class="simple">
<li><p>Loss might oscillate wildly (too high learning rate)</p></li>
<li><p>Training might take forever (too low learning rate)</p></li>
<li><p>Model might get stuck (no momentum when needed)</p></li>
<li><p>Like either tumbling down the mountain or never making it to the bottom</p></li>
</ul>
<p>Remember: Just like becoming a good skier requires practice and adjusting your technique based on the slope, finding the right learning rate and momentum often requires experimentation based on your specific model and data. Start conservative (lower learning rate, moderate momentum) and adjust based on how your model performs!</p>
<section id="id19">
<h3>Exercise<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<p>Experimenting with learning rate
In this exercise, your goal is to find the optimal learning rate such that the optimizer can find the minimum of the non-convex function x^4 + x^3 - 5x^2 in ten steps.</p>
<p>You will experiment with three different learning rate values. For this problem, try learning rate values between 0.001 to 0.1.</p>
<p>You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 10 steps of the SGD optimizer and display the results.</p>
</section>
<section id="id20">
<h3>Instructions (1/3)<a class="headerlink" href="#id20" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Try a small learning rate value such that the optimizer isn’t able to get past the first minimum on the right.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Try a first learning rate value</span>
<span class="c1"># lr0 = 0.005</span>
<span class="c1"># optimize_and_plot(lr=lr0)</span>
</pre></div>
</div>
</div>
</div>
<p>Instructions (1/3)</p>
<ul class="simple">
<li><p>Try a large learning rate value such that the optimizer skips past the global minimum at -2.</p></li>
</ul>
</section>
<section id="id21">
<h3>Exercise<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
</section>
<section id="experimenting-with-momentum">
<h3>Experimenting with momentum<a class="headerlink" href="#experimenting-with-momentum" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In this exercise, your goal is to find the optimal momentum such that the optimizer can find the minimum of the following non-convex function in 20 steps. You will experiment with two different momentum values. For this problem, the learning rate is fixed at 0.01.</p></li>
</ul>
<p>You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 20 steps of the SGD optimizer and display the results.</p>
</section>
<section id="id22">
<h3>Instructions (1/2)<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Try a first value for the momentum such that the optimizer gets stuck in the first minimum.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Try a first value for momentum</span>
<span class="c1"># mom0 = 0</span>
<span class="c1"># optimize_and_plot(momentum=mom0)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id23">
<h3>Instructions (2/2)<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Try a second value for momentum such that the optimizer finds the global optimum.</p></li>
</ul>
</section>
</section>
<section id="layer-initialization-and-transfer-learning">
<h2>Layer Initialization and Transfer Learning<a class="headerlink" href="#layer-initialization-and-transfer-learning" title="Link to this heading">#</a></h2>
</section>
<hr class="docutils" />
<section id="layer-initialization">
<h2>Layer Initialization<a class="headerlink" href="#layer-initialization" title="Link to this heading">#</a></h2>
<p>Think of layer initialization like setting up a new video game character. When you start a new game, your character needs some basic stats (like strength, speed, health) to begin their journey.</p>
<p><strong>Why It’s Important</strong>
Imagine trying to play a video game where your character starts with completely random abilities - sometimes super strong, sometimes super weak. That would make the game very hard to play! Similarly, in neural networks, if we don’t initialize our layers properly, our model might:</p>
<ul class="simple">
<li><p>Learn too slowly (like a character that’s too weak to fight)</p></li>
<li><p>Get stuck (like a character that can’t move)</p></li>
<li><p>Explode in values (like a character with infinite power that breaks the game)</p></li>
</ul>
</section>
<section id="transfer-learning">
<h2>Transfer Learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h2>
<p>Now, this is like having an experienced older sibling teach you how to play a video game!</p>
<p><strong>The Basic Concept</strong>
Imagine your sibling has already mastered a racing game. When you want to learn a new, similar racing game:</p>
<ul class="simple">
<li><p>They can teach you the basic controls</p></li>
<li><p>Show you how to take turns</p></li>
<li><p>Explain racing strategies</p></li>
<li><p>You just need to learn the specific details of the new game</p></li>
</ul>
<p><strong>How It Works in Machine Learning</strong></p>
<ol class="arabic simple">
<li><p><strong>Pre-trained Model</strong>: This is like your experienced sibling (the model that already knows a lot)</p></li>
<li><p><strong>New Task</strong>: This is your new game (the problem you want to solve)</p></li>
<li><p><strong>Transfer</strong>: You take what the pre-trained model knows and adapt it to your needs</p></li>
</ol>
<p><strong>Real-world Example</strong>
Let’s say you want to build a model that recognizes different types of cars. Instead of starting from scratch:</p>
<ol class="arabic simple">
<li><p>Take a model that’s already trained to recognize objects in general</p></li>
<li><p>Keep its basic knowledge (like understanding edges, shapes, colors)</p></li>
<li><p>Teach it specifically about cars</p></li>
</ol>
<p>This is much faster and more efficient than starting from zero - just like learning a game from an experienced player is faster than figuring everything out by yourself!</p>
<p><strong>When to Use It</strong>
Transfer learning is super helpful when:</p>
<ul class="simple">
<li><p>You don’t have lots of data (like getting game tips when you’re just starting)</p></li>
<li><p>You want to save training time (like skipping the basic tutorials)</p></li>
<li><p>Your task is similar to something that’s already been solved (like moving from one racing game to another)</p></li>
</ul>
<p>Remember, just like in video games, you don’t always have to start from level 1. Sometimes, you can build upon what others have already learned to reach your goals faster and more efficiently!</p>
</section>
<section id="fine-tuning-process">
<h2>Fine-tuning process<a class="headerlink" href="#fine-tuning-process" title="Link to this heading">#</a></h2>
<p>You are training a model on a new dataset and you think you can use a fine-tuning approach instead of training from scratch (i.e., training from randomly initialized weights).</p>
<p>To fine-tune a model on a new task, you need to follow a certain number of steps that are listed here.</p>
<ul class="simple">
<li><p><strong>Step 1:</strong> Find a model trained on a similar task</p></li>
<li><p><strong>Step 2:</strong> Load pre-trained weights</p></li>
<li><p><strong>Step 3:</strong> Freeze (or not) some of the layers in the model</p></li>
<li><p><strong>Step 4:</strong> Train with a smaller learning rate</p></li>
<li><p><strong>Step 5:</strong> Look at the loss values and see if the learning rate needs to be adjusted</p></li>
</ul>
<section id="code-example-of-freezing-layers-of-a-model">
<h3>Code example of Freezing layers of a model<a class="headerlink" href="#code-example-of-freezing-layers-of-a-model" title="Link to this heading">#</a></h3>
<p>You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.</p>
<p>The model has already been created and exists under the variable model. You will be using the named_parameters method of the model to list the parameters of the model. Each parameter is described by a name. This name is a string with the following naming convention: <a class="reference external" href="http://x.name">x.name</a> where x is the index of the layer.</p>
<p>Remember that a linear layer has two parameters: the weight and the bias. We are going to freeze the parameters of the first two layers of this model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  
    <span class="c1"># Check if the parameters belong to the first layer</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;0.weight&#39;</span> <span class="ow">or</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;0.bias&#39;</span><span class="p">:</span>
   
        <span class="c1"># Freeze the parameters</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        
    <span class="c1"># Check if the parameters belong to the second layer</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;1.weight&#39;</span> <span class="ow">or</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;1.bias&#39;</span><span class="p">:</span>
      
        <span class="c1"># Freeze the parameters</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<blockquote>
<div><p>Choosing which layer to freeze is an empirical process but a good rule of thumb is to start with the first layers and go deeper.</p>
</div></blockquote>
</section>
</section>
<section id="id24">
<h2>Exercise<a class="headerlink" href="#id24" title="Link to this heading">#</a></h2>
<section id="id25">
<h3>Layer initialization<a class="headerlink" href="#id25" title="Link to this heading">#</a></h3>
<p>The initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.</p>
<p>As a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer’s weights with the uniform method.</p>
</section>
<section id="id26">
<h3>Instructions<a class="headerlink" href="#id26" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>For each layer (<code class="docutils literal notranslate"><span class="pre">layer0</span></code>and <code class="docutils literal notranslate"><span class="pre">layer1</span></code>), use the uniform initialization method to initialize the weights.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="c1"># Use uniform initialization for layer0 and layer1 weights</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">layer0</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">layer1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer0</span><span class="p">,</span> <span class="n">layer1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="loading-data">
<h1>Loading Data<a class="headerlink" href="#loading-data" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Create a dictionary with the data</span>
<span class="n">animals</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;animal_name&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;skimmer&#39;</span><span class="p">,</span> <span class="s1">&#39;gull&#39;</span><span class="p">,</span> <span class="s1">&#39;seahorse&#39;</span><span class="p">,</span> <span class="s1">&#39;tuatara&#39;</span><span class="p">,</span> <span class="s1">&#39;squirrel&#39;</span><span class="p">],</span>
    <span class="s1">&#39;hair&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;feathers&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;eggs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;milk&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;predator&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;fins&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;legs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="s1">&#39;tail&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Create a DataFrame from the dictionary</span>
<span class="n">animals</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">animals</span><span class="p">)</span>

<span class="c1"># Display the DataFrame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">animals</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  animal_name  hair  feathers  eggs  milk  predator  fins  legs  tail  type
0     skimmer     0         1     1     0         1     0     2     1     2
1        gull     0         1     1     0         1     0     2     1     2
2    seahorse     0         0     1     0         0     1     0     1     4
3     tuatara     0         0     1     0         1     0     4     1     3
4    squirrel     1         0     0     1         0     0     2     1     1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define input features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">animals</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0 1 1 0 1 0 2 1]
 [0 1 1 0 1 0 2 1]
 [0 0 1 0 0 1 0 1]
 [0 0 1 0 1 0 4 1]
 [1 0 0 1 0 0 2 1]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define target features (ground truth)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">animals</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>

<span class="c1"># Instantiate dataset class</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

<span class="c1"># Access an individual sample</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_sample</span><span class="p">,</span> <span class="n">label_sample</span> <span class="o">=</span> <span class="n">sample</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;input sample:&#39;</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;label_sample:&#39;</span><span class="p">,</span> <span class="n">label_sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input sample: tensor([0., 1., 1., 0., 1., 0., 2., 1.])
label_sample: tensor(2.)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Create a DataLoader</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>

<span class="c1"># Iterate over the dataloader</span>
<span class="k">for</span> <span class="n">batch_inputs</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;batch inputs&#39;</span><span class="p">,</span> <span class="n">batch_inputs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;batch labels&#39;</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>batch inputs tensor([[0., 0., 1., 0., 0., 1., 0., 1.],
        [0., 1., 1., 0., 1., 0., 2., 1.]])
batch labels tensor([4., 2.])
batch inputs tensor([[0., 0., 1., 0., 1., 0., 4., 1.],
        [1., 0., 0., 1., 0., 0., 2., 1.]])
batch labels tensor([3., 1.])
batch inputs tensor([[0., 1., 1., 0., 1., 0., 2., 1.]])
batch labels tensor([2.])
</pre></div>
</div>
</div>
</div>
<section id="id27">
<h2>Exercise<a class="headerlink" href="#id27" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>Using the TensorDataset class
In practice, loading your data into a PyTorch dataset will be one of the first steps you take in order to create and train a neural network with PyTorch.</p>
<p>The TensorDataset class is very helpful when your dataset can be loaded directly as a NumPy array. Recall that TensorDataset() can take one or more NumPy arrays as input.</p>
<p>In this exercise, you’ll practice creating a PyTorch dataset using the TensorDataset class.</p>
<p>torch and numpy have already been imported for you, along with the TensorDataset class.</p>
<section id="id28">
<h3>Instructions<a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Convert the NumPy arrays provided to PyTorch tensors.</p></li>
<li><p>Create a TensorDataset using the torch_features and the torch_target tensors provided (in this order).</p></li>
<li><p>Return the last element of the dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>

<span class="n">np_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">np_target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Convert arrays to PyTorch tensors</span>
<span class="n">torch_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np_features</span><span class="p">)</span>
<span class="n">torch_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np_target</span><span class="p">)</span>

<span class="c1"># Create a TensorDataset from two tensors</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch_features</span><span class="p">,</span> <span class="n">torch_target</span><span class="p">)</span>

<span class="c1"># Return the last element of this dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.2829, 0.2822, 0.0770, 0.3640, 0.9413, 0.4576, 0.7154, 0.0449],
       dtype=torch.float64), tensor([0.4225], dtype=torch.float64))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id29">
<h2>Exercise<a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<section id="from-data-loading-to-running-a-forward-pass">
<h3>From data loading to running a forward pass<a class="headerlink" href="#from-data-loading-to-running-a-forward-pass" title="Link to this heading">#</a></h3>
<p>In this exercise, you’ll create a PyTorch DataLoader from a pandas DataFrame and call a model on this dataset. Specifically, you’ll run a forward pass on a neural network. You’ll continue working with fully connected neural networks, as you have done so far.</p>
<p>You’ll begin by subsetting a loaded DataFrame called dataframe, converting features and targets NumPy arrays, and converting to PyTorch tensors in order to create a PyTorch dataset.</p>
<p>This dataset can be loaded into a PyTorch DataLoader, batched, shuffled, and used to run a forward pass on a custom fully connected neural network.</p>
<p>NumPy as np, pandas as pd, torch, TensorDataset(), and DataLoader() have been imported for you</p>
</section>
<section id="id30">
<h3>Instructions (1/3)<a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<p>Extract the features (ph, Sulfate, Conductivity, Organic_carbon) and target (Potability) values and load them into the appropriate tensors to represent features and targets.
Use both tensors to create a PyTorch dataset using the dataset class that’s quickest to use when tensors don’t require any additional preprocessing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Load the different columns into two PyTorch tensors</span>
<span class="c1"># features = torch.tensor(dataframe[[&#39;ph&#39;, &#39;Sulfate&#39;, &#39;Conductivity&#39;, &#39;Organic_carbon&#39;]].to_numpy()).float()</span>
<span class="c1"># target = torch.tensor(dataframe[&#39;Potability&#39;].to_numpy()).float()</span>

<span class="c1"># # Create a dataset from the two generated tensors</span>
<span class="c1"># dataset = TensorDataset(features, target)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id31">
<h3>Instructions 2/3<a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<p>Create a PyTorch DataLoader from the created TensorDataset; this DataLoader should use a batch_size of two and shuffle the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Load the different columns into two PyTorch tensors</span>
<span class="c1"># features = torch.tensor(dataframe[[&#39;ph&#39;, &#39;Sulfate&#39;, &#39;Conductivity&#39;, &#39;Organic_carbon&#39;]].to_numpy()).float()</span>
<span class="c1"># target = torch.tensor(dataframe[&#39;Potability&#39;].to_numpy()).float()</span>

<span class="c1"># # Create a dataset from the two generated tensors</span>
<span class="c1"># dataset = TensorDataset(features, target)</span>

<span class="c1"># # Create a dataloader using the above dataset</span>
<span class="c1"># dataloader = DataLoader(dataset, batch_size=2, shuffle=True)</span>
<span class="c1"># x, y = next(iter(dataloader))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id32">
<h3>Instructions 3/3<a class="headerlink" href="#id32" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Implement a small, fully connected neural network using exactly two linear layers and the nn.Sequential() API, where the final output size is 1.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Load the different columns into two PyTorch tensors</span>
<span class="c1"># features = torch.tensor(dataframe[[&#39;ph&#39;, &#39;Sulfate&#39;, &#39;Conductivity&#39;, &#39;Organic_carbon&#39;]].to_numpy()).float()</span>
<span class="c1"># target = torch.tensor(dataframe[&#39;Potability&#39;].to_numpy()).float()</span>

<span class="c1"># # Create a dataset from the two generated tensors</span>
<span class="c1"># dataset = TensorDataset(features, target)</span>

<span class="c1"># # Create a dataloader using the above dataset</span>
<span class="c1"># dataloader = DataLoader(dataset, shuffle=True, batch_size=2)</span>
<span class="c1"># x, y = next(iter(dataloader))</span>

<span class="c1"># # Create a model using the nn.Sequential API</span>
<span class="c1"># model = nn.Sequential(</span>
<span class="c1">#   nn.Linear(4, 16), </span>
<span class="c1">#   nn.Linear(16, 1)</span>
<span class="c1"># )</span>

<span class="c1"># output = model(features)</span>
<span class="c1"># print(output)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-evaluation">
<h1>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<section id="the-three-stages-of-learning-training-validation-and-testing">
<h2>The Three Stages of Learning: Training, Validation and Testing<a class="headerlink" href="#the-three-stages-of-learning-training-validation-and-testing" title="Link to this heading">#</a></h2>
<p>Imagine you have a big cookbook with 100 recipes. Here’s how we’d split them to teach Tommy:</p>
<p><strong>Training Set (60-80 recipes)</strong>
Think of this as Tommy’s practice kitchen. This is where he learns and makes mistakes:</p>
<ul class="simple">
<li><p>Tommy practices cooking these recipes over and over</p></li>
<li><p>He learns the basic techniques</p></li>
<li><p>He burns some dishes and overcooks others</p></li>
<li><p>Just like how our ML model learns patterns from training data</p></li>
</ul>
<p><strong>Validation Set (10-20 recipes)</strong>
This is like having Tommy cook for his family before the big restaurant opening:</p>
<ul class="simple">
<li><p>His family gives feedback on the taste</p></li>
<li><p>He adjusts his cooking style based on their comments</p></li>
<li><p>He might realize he’s using too much salt or cooking too fast</p></li>
<li><p>Similarly, we use validation data to fine-tune our model’s settings (hyperparameters)</p></li>
</ul>
<p><strong>Test Set (10-20 recipes)</strong>
This is like Tommy’s final exam in cooking school:</p>
<ul class="simple">
<li><p>These are completely new recipes he’s never seen</p></li>
<li><p>He can’t get feedback or make adjustments</p></li>
<li><p>This shows if he truly learned to cook or just memorized recipes</p></li>
<li><p>Just like how we use test data to see if our model can handle new, unseen data</p></li>
</ul>
</section>
<section id="why-this-split-is-important">
<h2>Why This Split is Important<a class="headerlink" href="#why-this-split-is-important" title="Link to this heading">#</a></h2>
<p>Think of it this way:</p>
<ul class="simple">
<li><p>If Tommy only practiced on the same 3 recipes (too little training data), he wouldn’t become a good chef</p></li>
<li><p>If he got feedback on his final exam recipes (using test data for training), it wouldn’t be a fair test</p></li>
<li><p>If his family gave him too many tips during practice (overfitting to validation data), he might not learn to cook independently</p></li>
</ul>
</section>
<section id="real-world-example">
<h2>Real-World Example<a class="headerlink" href="#real-world-example" title="Link to this heading">#</a></h2>
<p>Just like how a student can’t prepare for an exam using the actual test questions, our model shouldn’t see the test data during training. Imagine if Tommy memorized exactly how to make a chocolate cake, but then the test asks him to make a vanilla cake - he needs to understand the general principles of baking, not just memorize one recipe.</p>
<p>This three-way split helps ensure our model (like Tommy) truly learns to generalize and can handle new, unseen situations rather than just memorizing the training data.</p>
</section>
<section id="id33">
<h2>Exercise<a class="headerlink" href="#id33" title="Link to this heading">#</a></h2>
<section id="writing-the-evaluation-loop">
<h3>Writing the evaluation loop<a class="headerlink" href="#writing-the-evaluation-loop" title="Link to this heading">#</a></h3>
<p>In this exercise, you will practice writing the evaluation loop. Recall that the evaluation loop is similar to the training loop, except that you will not perform the gradient calculation and the optimizer step.</p>
<p>The model has already been defined for you, along with the object validationloader, which is a dataset.</p>
</section>
<section id="id34">
<h3>Instructions (1/2)<a class="headerlink" href="#id34" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Set the model to evaluation mode.</p></li>
<li><p>Sum the current batch loss to the validation_loss variable.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Set the model to evaluation mode</span>
<span class="c1"># model.eval()</span>
<span class="c1"># validation_loss = 0.0</span>

<span class="c1"># with torch.no_grad():</span>
  
<span class="c1">#   for data in validationloader:</span>
    
<span class="c1">#       outputs = model(data[0])</span>
<span class="c1">#       loss = criterion(outputs, data[1])</span>
      
<span class="c1">#       # Sum the current loss to the validation_loss variable</span>
<span class="c1">#       validation_loss += loss.item()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id35">
<h3>Instructions (2/2)<a class="headerlink" href="#id35" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Calculate the mean loss value for the epoch.</p></li>
<li><p>Set the model back to training mode.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Set the model to evaluation mode</span>
<span class="c1"># model.eval()</span>
<span class="c1"># validation_loss = 0.0</span>

<span class="c1"># with torch.no_grad():</span>
  
<span class="c1">#   for data in validationloader:</span>
    
<span class="c1">#       outputs = model(data[0])</span>
<span class="c1">#       loss = criterion(outputs, data[1])</span>
      
<span class="c1">#       # Sum the current loss to the validation_loss variable</span>
<span class="c1">#       validation_loss += loss.item()</span>
      
<span class="c1"># # Calculate the mean loss value</span>
<span class="c1"># validation_loss_epoch = validation_loss / len(validationloader)</span>
<span class="c1"># print(validation_loss_epoch)</span>

<span class="c1"># # Set the model back to training mode</span>
<span class="c1"># model.train()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id36">
<h2>Exercise<a class="headerlink" href="#id36" title="Link to this heading">#</a></h2>
<section id="calculating-accuracy-using-torchmetrics">
<h3>Calculating accuracy using torchmetrics<a class="headerlink" href="#calculating-accuracy-using-torchmetrics" title="Link to this heading">#</a></h3>
<p>In addition to the losses, you should also be keeping track of the accuracy during training. By doing so, you will be able to select the epoch when the model performed the best.</p>
<p>In this exercise, you will practice using the torchmetrics package to calculate the accuracy. You will be using a sample of the facemask dataset. This dataset contains three different classes. The plot_errors function will display samples where the model predictions do not match the ground truth. Performing such error analysis will help you understand your model failure modes.</p>
<p>The torchmetrics package is already imported. The model outputs are the probabilities returned by a softmax as the last step of the model. The labels tensor contains the labels as one-hot encoded vectors.</p>
</section>
<section id="id37">
<h3>Instructions (1/2)<a class="headerlink" href="#id37" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create an accuracy metric for a “multiclass” problem with three classes.</p></li>
<li><p>Calculate the accuracy for each batch of the dataloader.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Create accuracy metric using torch metrics</span>
<span class="c1"># metric = torchmetrics.Accuracy(task=&quot;multiclass&quot;, num_classes=3)</span>
<span class="c1"># for data in dataloader:</span>
<span class="c1">#     features, labels = data</span>
<span class="c1">#     outputs = model(features)</span>
    
<span class="c1">#     # Calculate accuracy over the batch</span>
<span class="c1">#     acc = metric(outputs, labels.argmax(dim=-1))</span>
<span class="c1"># acc</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id38">
<h3>Instructions (2/2)<a class="headerlink" href="#id38" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Calculate accuracy for the epoch.</p></li>
<li><p>Reset the metric for the next epoch.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Create accuracy metric using torch metrics</span>
<span class="c1"># metric = torchmetrics.Accuracy(task=&quot;multiclass&quot;, num_classes=3)</span>
<span class="c1"># for data in dataloader:</span>
<span class="c1">#     features, labels = data</span>
<span class="c1">#     outputs = model(features)</span>
    
<span class="c1">#     # Calculate accuracy over the batch</span>
<span class="c1">#     acc = metric(outputs, labels.argmax(dim=-1))</span>
    
<span class="c1"># # Calculate accuracy over the whole epoch</span>
<span class="c1"># acc = metric.compute()</span>

<span class="c1"># # Reset the metric for the next epoch (training or validation)</span>
<span class="c1"># metric.reset()</span>
<span class="c1"># plot_errors(model, dataloader)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="overfitting-and-underfitting-models">
<h1>Overfitting and Underfitting Models<a class="headerlink" href="#overfitting-and-underfitting-models" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<section id="the-story-of-three-students">
<h2>The Story of Three Students<a class="headerlink" href="#the-story-of-three-students" title="Link to this heading">#</a></h2>
<p>Imagine three students preparing for their math tests: Mary (who overfits), Johnny (who underfits), and Sarah (who gets it just right).</p>
</section>
<section id="overfitting-meet-mary-the-memorizer">
<h2>Overfitting: Meet Mary the Memorizer<a class="headerlink" href="#overfitting-meet-mary-the-memorizer" title="Link to this heading">#</a></h2>
<p>Think of Mary as a student who <strong>memorizes everything</strong> without understanding the concepts:</p>
<ul class="simple">
<li><p>She memorizes every single math problem from the textbook</p></li>
<li><p>She knows that “2 + 3 = 5” and “4 + 6 = 10”</p></li>
<li><p>But when she sees “7 + 8”, she’s stuck because she never memorized this exact problem</p></li>
</ul>
<p>This is like overfitting in machine learning where:</p>
<ul class="simple">
<li><p>The model learns the training data too perfectly</p></li>
<li><p>It memorizes even the noise and random fluctuations</p></li>
<li><p>It performs great on known data but fails on new problems</p></li>
<li><p>It’s like having a GPS that only knows one route to school and gets lost if there’s road construction</p></li>
</ul>
<p><strong>Signs of Overfitting:</strong></p>
<ul class="simple">
<li><p>Perfect performance on training data</p></li>
<li><p>Poor performance on new data</p></li>
<li><p>The model becomes too complex and specific</p></li>
</ul>
</section>
<section id="underfitting-meet-johnny-the-generalizer">
<h2>Underfitting: Meet Johnny the Generalizer<a class="headerlink" href="#underfitting-meet-johnny-the-generalizer" title="Link to this heading">#</a></h2>
<p>Johnny represents the opposite problem:</p>
<ul class="simple">
<li><p>He only learns that “numbers can be added together”</p></li>
<li><p>He doesn’t practice enough problems</p></li>
<li><p>He doesn’t learn any specific techniques or patterns</p></li>
</ul>
<p>This is like underfitting in machine learning where:</p>
<ul class="simple">
<li><p>The model is too simple</p></li>
<li><p>It doesn’t capture important patterns in the data</p></li>
<li><p>It performs poorly on both training and new data</p></li>
<li><p>It’s like having a GPS that only knows “go north” or “go south” without understanding streets</p></li>
</ul>
<p><strong>Signs of Underfitting:</strong></p>
<ul class="simple">
<li><p>Poor performance on training data</p></li>
<li><p>Poor performance on new data</p></li>
<li><p>The model is too simple and misses obvious patterns</p></li>
</ul>
</section>
<section id="just-right-meet-sarah-the-smart-learner">
<h2>Just Right: Meet Sarah the Smart Learner<a class="headerlink" href="#just-right-meet-sarah-the-smart-learner" title="Link to this heading">#</a></h2>
<p>Sarah represents the perfect balance:</p>
<ul class="simple">
<li><p>She learns the rules and patterns of math</p></li>
<li><p>She understands why “2 + 3 = 5”</p></li>
<li><p>She can solve new problems using her understanding</p></li>
<li><p>She practices enough but doesn’t just memorize</p></li>
</ul>
<p>This is the sweet spot we want in machine learning where:</p>
<ul class="simple">
<li><p>The model learns genuine patterns</p></li>
<li><p>It ignores random noise in the data</p></li>
<li><p>It performs well on both old and new data</p></li>
<li><p>It’s like having a GPS that knows multiple routes and can handle detours</p></li>
</ul>
</section>
<section id="why-it-matters">
<h2>Why It Matters<a class="headerlink" href="#why-it-matters" title="Link to this heading">#</a></h2>
<p>Finding the right balance is crucial because:</p>
<ul class="simple">
<li><p>Too simple (underfitting) = missing important patterns</p></li>
<li><p>Too complex (overfitting) = learning noise instead of patterns</p></li>
<li><p>Just right = model that can generalize and make good predictions</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="fighting-overfitting">
<h1>Fighting Overfitting<a class="headerlink" href="#fighting-overfitting" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p>Here are three of the many techniques for fighting overfitting:</p>
<section id="weight-decay-the-lazy-student-method">
<h2>Weight Decay: The “Lazy Student” Method<a class="headerlink" href="#weight-decay-the-lazy-student-method" title="Link to this heading">#</a></h2>
<p>Think of weight decay like forcing a student to take the simplest path to solve a problem:</p>
<ul class="simple">
<li><p>Without weight decay: A student writes a 10-page solution for a simple addition problem</p></li>
<li><p>With weight decay: The student is encouraged to write shorter, simpler solutions</p></li>
</ul>
<p>It’s like telling the model: “Don’t try too hard!”</p>
<ul class="simple">
<li><p>It penalizes large weights in the model</p></li>
<li><p>Makes the model prefer simpler solutions</p></li>
<li><p>Like Occam’s Razor: the simplest solution is often the best</p></li>
</ul>
</section>
<section id="data-augmentation-the-study-buddy-method">
<h2>Data Augmentation: The “Study Buddy” Method<a class="headerlink" href="#data-augmentation-the-study-buddy-method" title="Link to this heading">#</a></h2>
<p>Imagine you’re studying for a test:</p>
<ul class="simple">
<li><p>Original way: You only study the exact questions in your textbook</p></li>
<li><p>Augmented way: You study similar questions with slight variations</p></li>
</ul>
<p>It’s like showing your model the same picture in different ways:</p>
<ul class="simple">
<li><p>Flip the image</p></li>
<li><p>Rotate it slightly</p></li>
<li><p>Change the brightness</p></li>
<li><p>Add some noise</p></li>
</ul>
<p>Real-world example:</p>
<ul class="simple">
<li><p>Teaching a model to recognize cats</p></li>
<li><p>Original: Show one picture of a cat</p></li>
<li><p>Augmented: Show the same cat upside down, slightly blurry, in different lighting</p></li>
<li><p>This helps the model learn “cat-ness” rather than memorizing specific cat pictures</p></li>
</ul>
</section>
<section id="dropout-the-pop-quiz-method">
<h2>Dropout: The “Pop Quiz” Method<a class="headerlink" href="#dropout-the-pop-quiz-method" title="Link to this heading">#</a></h2>
<p>Think of dropout like randomly covering parts of your textbook while studying:</p>
<ul class="simple">
<li><p>Each time you practice, some random words are covered up</p></li>
<li><p>You learn to understand the full concept without relying on specific words</p></li>
</ul>
<p>It’s like training your brain to work with incomplete information:</p>
<ul class="simple">
<li><p>During training, randomly turn off some neurons</p></li>
<li><p>Forces the model to not rely too heavily on any single feature</p></li>
<li><p>Like having multiple backup plans</p></li>
</ul>
<p>Real-world example:</p>
<ul class="simple">
<li><p>Learning to recognize a car</p></li>
<li><p>Without dropout: The model might only look for wheels</p></li>
<li><p>With dropout: The model learns to look for wheels, windows, doors, etc.</p></li>
<li><p>If it can’t see the wheels, it can still recognize the car from other features</p></li>
</ul>
</section>
<section id="why-use-all-three">
<h2>Why Use All Three?<a class="headerlink" href="#why-use-all-three" title="Link to this heading">#</a></h2>
<p>Think of it like training for a sport:</p>
<ul class="simple">
<li><p><strong>Weight Decay</strong>: Don’t over-complicate your moves</p></li>
<li><p><strong>Data Augmentation</strong>: Practice in different conditions (rain, sun, wind)</p></li>
<li><p><strong>Dropout</strong>: Train different muscle groups so you don’t rely on just one</p></li>
</ul>
<p>Together, these techniques help your model become more robust and generalize better, just like a well-rounded athlete who can perform in any condition!</p>
</section>
<section id="id39">
<h2>Exercise<a class="headerlink" href="#id39" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="experimenting-with-dropout">
<h3>Experimenting with dropout<a class="headerlink" href="#experimenting-with-dropout" title="Link to this heading">#</a></h3>
<p>The dropout layer randomly zeroes out elements of the input tensor. Doing so helps fight overfitting. In this exercise, you’ll create a small neural network with one linear layer, one dropout layer, and one activation function.</p>
<p>The torch.nn package has already been imported as nn. An input_tensor of dimensions
has been created for you.</p>
</section>
<section id="id40">
<h3>Instructions (1/2)<a class="headerlink" href="#id40" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Create a small neural network with one linear layer, one ReLU function, and one dropout layer, in that order.
The model should take input_tensor as input and return an output of size 16.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Create a small neural network</span>
<span class="c1"># model = nn.Sequential(nn.Linear(3072, 16),</span>
<span class="c1">#                       nn.ReLU(),</span>
<span class="c1">#                       nn.Dropout())</span>
<span class="c1"># model(input_tensor)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id41">
<h3>Instructions (2/2)<a class="headerlink" href="#id41" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Using the same neural network, set the probability of zeroing out elements in the dropout layer to 0.8.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Using the same model, set the dropout probability to 0.8</span>
<span class="c1"># model = nn.Sequential(nn.Linear(3072, 16),</span>
<span class="c1">#                       nn.ReLU(),</span>
<span class="c1">#                       nn.Dropout(p=0.8))</span>
<span class="c1"># model(input_tensor)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="improving-model-performance">
<h1>Improving Model Performance<a class="headerlink" href="#improving-model-performance" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<section id="step-1-establish-a-baseline">
<h2>Step 1: Establish a Baseline<a class="headerlink" href="#step-1-establish-a-baseline" title="Link to this heading">#</a></h2>
<p>Think of this like learning to play basketball. Before trying fancy moves, you want to make sure you can make basic shots when practicing alone. In machine learning:</p>
<ul class="simple">
<li><p>First, let your model practice extensively on your training data</p></li>
<li><p>If it can’t perform well even on the practice data, it’s like not being able to make shots in an empty gym</p></li>
<li><p>This tells you if your “player” (model) has the basic ability to learn the task</p></li>
</ul>
</section>
<section id="step-2-reduce-overfitting">
<h2>Step 2: Reduce Overfitting<a class="headerlink" href="#step-2-reduce-overfitting" title="Link to this heading">#</a></h2>
<p>This is like making sure you can play well in real games, not just practice. Imagine a player who only knows how to shoot from one specific spot - they’re “overfitting” to that position:</p>
<ul class="simple">
<li><p>Add “training rules” (regularization) to make your model more flexible</p></li>
<li><p>Watch how it performs in “practice games” (validation set)</p></li>
<li><p>Make sure it can handle new situations it hasn’t seen before</p></li>
</ul>
</section>
<section id="step-3-optimize-hyperparameters">
<h2>Step 3: Optimize Hyperparameters<a class="headerlink" href="#step-3-optimize-hyperparameters" title="Link to this heading">#</a></h2>
<p>This is like fine-tuning your training routine:</p>
<ul class="simple">
<li><p>Adjusting how fast you learn (learning rate)</p></li>
<li><p>Deciding how many practice shots to take before getting feedback (batch size)</p></li>
<li><p>Choosing the right training equipment and exercises (network architecture)</p></li>
</ul>
<p>The Continuous Process:
Just like in sports, getting better is an ongoing process. You might need to:</p>
<ul class="simple">
<li><p>Practice with more different situations (more training data)</p></li>
<li><p>Try different training techniques (different model architectures)</p></li>
<li><p>Keep track of your progress and adjust your training plan</p></li>
</ul>
<p>Remember: The key is to first make sure your model can learn at all, then make sure it can handle new situations, and finally fine-tune it to perform its best. It’s like learning any skill - start with the basics, make sure you can apply them broadly, and then work on perfecting your technique.</p>
</section>
<section id="id42">
<h2>Exercise<a class="headerlink" href="#id42" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="implementing-random-search">
<h3>Implementing random search<a class="headerlink" href="#implementing-random-search" title="Link to this heading">#</a></h3>
<p>Hyperparameter search is a computationally costly approach to experiment with different hyperparameter values. However, it can lead to performance improvements. In this exercise, you will implement a random search algorithm.</p>
<p>You will randomly sample 10 values of the learning rate and momentum from the uniform distribution. To do so, you will use the np.random.uniform() function.</p>
<p>The numpy package has already been imported as np.</p>
</section>
<section id="id43">
<h3>Instructions<a class="headerlink" href="#id43" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Randomly sample a learning rate factor between 2 and 4 so that the learning rate (lr) is bounded between 10^-2 and 10^-4.</p></li>
<li><p>Randomly sample a momentum between 0.85 and 0.99.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># values = []</span>
<span class="c1"># for idx in range(10):</span>
<span class="c1">#     # Randomly sample a learning rate factor 2 and 4</span>
<span class="c1">#     factor = np.random.uniform(2, 4)</span>
<span class="c1">#     lr = 10 ** -factor</span>
    
<span class="c1">#     # Randomly sample a momentum between 0.85 and 0.99</span>
<span class="c1">#     momentum = np.random.uniform(0.85, 0.99)</span>
    
<span class="c1">#     values.append((lr, momentum))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Chapter%204%20-%20Unsupervised%20Learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 4 - Unsupervised Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="Chapter%206%20-%20Deep%20Learning%20Tools.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 6 - Deep Learning Tools</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 5 - Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-overview">Neural Network Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-torch-library">Importing the Torch Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-attributes">Tensor Attributes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensor-attributes">What are tensor attributes?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shape">1. Tensor Shape</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-data-type">2. Tensor Data Type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-device">3. Tensor Device</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-our-own-neural-network">Creating our own Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-to-know-the-linear-layer-operation">Getting to know the linear layer operation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-visualize-the-linear-layers-with-their-weights-and-biases">Lets visualize the Linear_layers with their weights and biases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ignore-the-code-block-below-it-is-just-for-visualization">Ignore the code block below, it is just for visualization</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#each-nn-linear-line-of-code-is-a-linear-layer">Each <strong>nn.Linear</strong> line of code is a Linear Layer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-layers-with-nn-sequential">Stacking layers with nn.Sequential()</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise #1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-first-neural-network">Your first neural network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-linear-layers">Stacking linear layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#there-are-multiple-types-of-layers">There are multiple types of Layers</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-activation-functions">Why do we need activation functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function-example-with-an-activation-layer">Sigmoid function example with an Activation layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function-limitations">Sigmoid Function Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-each">When to Use Each</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-as-an-activation-layer">Softmax as an Activation Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function-limitations">Softmax Function Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#better-alternatives">Better Alternatives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">When to Use Each</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-6">Exercise 1.6</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-and-softmax-functions">The sigmoid and softmax functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-1-2">Instructions 1/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-2-2">Instruction 2/2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-and-backpropagation">Forward Propagation and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-forward-propagation">What is Forward Propagation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-used-and-important">Why is it Used and Important?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-analogy-for-forward-propagation">An Analogy for Forward Propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-backpropagation">What is Backpropagation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Why is it Used and Important?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-analogy-for-backpropagation">An Analogy for Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-differences-between-terms">Understanding the Differences Between Terms</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-vs-regression-problems-in-deep-learning">Classification vs Regression Problems in Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-continuous-output">Regression (Continuous Output)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-discrete-output">Classification (Discrete Output)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-in-forward-pass">Key Differences in Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-real-world-example">Simple Real-World Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification-forward-propagation">Binary Classification: Forward Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-video-game-character-selection-analogy">The Video Game Character Selection Analogy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-station-input-layer">Input Station (Input Layer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magic-processing-stations-hidden-layers">Magic Processing Stations (Hidden Layers)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-decision-station-output-layer">Final Decision Station (Output Layer)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-classification-forward-pass">Multi-class classification: forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-school-club-assignment-system">The School Club Assignment System</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-assignment-process-forward-pass">The Assignment Process (Forward Pass)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-of-multi-class-classification-from-binary-classification">Key Differences of Multi-class classification from Binary Classification:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-s-important">Why It’s Important:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-forward-propogation">Regression: Forward Propogation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-1">Exercise 2.1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-binary-classifier-in-pytorch">Building a binary classifier in PyTorch</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-2">Exercise 2.2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-regression-to-multi-class-classification">From regression to multi-class classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Instructions 1/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-2-2">Instructions 2/2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-for-the-code-above">Explanation for the code above</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#using-loss-functions-to-assess-model-predictions">Using Loss Functions to assess model predictions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-loss-function">What is a Loss Function?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-loss-functions">Why Do We Need Loss Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-loss-functions">Types of Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-loss-functions">Regression Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-in-a-formula">Loss Function in a formula</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One Hot encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#here-s-how-it-works">Here’s how it works:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-use-one-hot-encoding">Why do we use One Hot encoding?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-takes">Loss function takes:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function-outputs">Loss function outputs:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#our-training-goal-is-to-minimize-this-loss-the-lower-the-loss-the-better-the-model">✨ <strong>Our training goal is to minimize this loss!</strong> The lower the loss, the better the model! ✨</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-3">Exercise 2.3</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-one-hot-encoded-labels">Creating one-hot encoded labels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-4">Exercise 2.4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-cross-entropy-loss">Calculating cross entropy loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-1-3">Instruction 1/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-2-3">Instruction 2/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-3-3">Instruction 3/3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#using-derivatives-to-update-model-parameters-a-treasure-hunt-analogy">Using Derivatives to Update Model Parameters: A Treasure Hunt Analogy</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-treasure-hunt-game-an-analogy-for-gradient-descent">The Treasure Hunt Game: An Analogy for Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#game-rules">Game Rules:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mapping-the-game-to-machine-learning-concepts">Mapping the Game to Machine Learning Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-park-the-loss-landscape">The Park = The “Loss Landscape”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#you-the-learning-algorithm">You = The Learning Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feeling-the-ground-calculating-gradients">Feeling the Ground = Calculating Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#taking-steps-updating-the-model">Taking Steps = Updating the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-treasure-the-best-model">The Treasure = The Best Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-concepts">Important Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-step-size">Learning Rate = Step Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-minima-small-dips">Local Minima = Small Dips</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-finding-the-treasure">Convergence = Finding the Treasure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-is-fascinating">Why this is Fascinating</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#below-is-an-example-of-backpropagation">Below is an example of Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-breakdown">Code Breakdown</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">1. Data Preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture">2. Model Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">3. Forward Pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-calculation-and-backpropagation">4. Loss Calculation and Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-gradients">5. Accessing Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-output-of-the-code-above">Understanding the Output of the code above ⬆︎</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-part-weight-gradients">First Part (Weight Gradients)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-part-bias-gradients">Second Part (Bias Gradients)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-analogy">Real-world Analogy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#most-popular-pytorch-optimizers">Most Popular PyTorch Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-optimizers">How to Use Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-optimizer">Choosing the Right Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-for-using-optimizers">Tips for Using Optimizers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-a-sample">Estimating a sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Exercise</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-the-model-parameters">Accessing the model parameters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-weights-manually">Updating the weights manually</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-pytorch-optimizer">Using the PyTorch optimizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-our-first-training-loop">Writing our first Training Loop!</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-neural-network">Training a neural network</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introducting-the-mean-squared-error-loss">Introducting the Mean Squared Error Loss</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-mse-with-a-pizza-delivery-analogy">Understanding MSE with a Pizza Delivery Analogy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-implementation">PyTorch Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-mseloss">Using the MSELoss</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#some-terminology-before-we-move-forward">Some terminology before we move forward</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epoch">Epoch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-features">batch_features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-labels">batch_labels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-dataloader">The dataloader</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-a-training-loop">Writing a training loop</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-1-3">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-2-3">Instructions (2/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions-3-3">Instructions (3/3)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-neural-network-architecture-and-hyperparameters">Section 3 - Neural Network Architecture and Hyperparameters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-relu-rectified-linear-unit-and-leakyrelu">Introducing ReLU (Rectified Linear Unit) and LeakyReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">Leaky ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-relu">Implementing ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-leaky-relu">Implementing leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-counting">Parameter Counting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-count-the-parameters-in-a-neural-network-using-a-simple-example">Let’s count the parameters in a neural network using a simple example:</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#here-s-the-code-we-ll-analyze">Here’s the code we’ll analyze:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-restaurant-kitchen-model">The Restaurant Kitchen Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Instructions (2/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-the-code-above-does">What the code above does</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-parameter-counter">Understanding the Parameter Counter</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-the-capacity-of-a-network">Manipulating the capacity of a network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-momentum-with-sgd-optimizer-stocahastic-gradient-descent">Learning Rate and Momentum with SGD Optimizer (Stocahastic Gradient Descent)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-skiing-adventure-of-sgd">The Skiing Adventure of SGD</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-your-skiing-speed">Learning Rate: Your Skiing Speed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-momentum-the-skiing-inertia">Understanding Momentum: The Skiing Inertia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-momentum-0-9-0-99">High Momentum (0.9 - 0.99)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#low-momentum-0-1-0-5">Low Momentum (0.1 - 0.5)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-momentum-0-0">Zero Momentum (0.0)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-scenarios">Real-World Scenarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-momentum-values-and-their-effects">Common Momentum Values and Their Effects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips">Practical Tips</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-right-balance">Finding the Right Balance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimenting-with-momentum">Experimenting with momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-initialization-and-transfer-learning">Layer Initialization and Transfer Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-initialization">Layer Initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-process">Fine-tuning process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example-of-freezing-layers-of-a-model">Code example of Freezing layers of a model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Layer initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Instructions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-data">Loading Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Instructions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-data-loading-to-running-a-forward-pass">From data loading to running a forward pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Instructions (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Instructions 2/3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Instructions 3/3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">Model Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-stages-of-learning-training-validation-and-testing">The Three Stages of Learning: Training, Validation and Testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-split-is-important">Why This Split is Important</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-example">Real-World Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-the-evaluation-loop">Writing the evaluation loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Instructions (2/2)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-accuracy-using-torchmetrics">Calculating accuracy using torchmetrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Instructions (2/2)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting-models">Overfitting and Underfitting Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-story-of-three-students">The Story of Three Students</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-meet-mary-the-memorizer">Overfitting: Meet Mary the Memorizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting-meet-johnny-the-generalizer">Underfitting: Meet Johnny the Generalizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#just-right-meet-sarah-the-smart-learner">Just Right: Meet Sarah the Smart Learner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-matters">Why It Matters</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#fighting-overfitting">Fighting Overfitting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-decay-the-lazy-student-method">Weight Decay: The “Lazy Student” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-augmentation-the-study-buddy-method">Data Augmentation: The “Study Buddy” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-the-pop-quiz-method">Dropout: The “Pop Quiz” Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-all-three">Why Use All Three?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimenting-with-dropout">Experimenting with dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Instructions (1/2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Instructions (2/2)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-model-performance">Improving Model Performance</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-establish-a-baseline">Step 1: Establish a Baseline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-reduce-overfitting">Step 2: Reduce Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-optimize-hyperparameters">Step 3: Optimize Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-random-search">Implementing random search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Instructions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shreyash Gupta
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>